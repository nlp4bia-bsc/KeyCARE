{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ecb69cf-31ea-4dc6-8e0c-d7596f13ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-ft1_zc_j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-ft1_zc_j\n",
      "  Resolved https://github.com/LIAAD/yake to commit 374fc1c1c19eb080d5b6115cbb8d4a4324392e54\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=6.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (8.1.6)\n",
      "Requirement already satisfied: jellyfish in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (1.0.0)\n",
      "Requirement already satisfied: networkx in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (3.1)\n",
      "Requirement already satisfied: numpy in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (1.25.2)\n",
      "Requirement already satisfied: segtok in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (1.5.11)\n",
      "Requirement already satisfied: tabulate in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (0.9.0)\n",
      "Requirement already satisfied: regex in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from segtok->yake==0.4.8) (2023.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19908a5e-4d69-47e7-9a33-137ab2a503d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: joblib in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
      "Requirement already satisfied: click in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb147fe-ed9d-4b84-ba89-b9ef65135ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytextrank in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (3.2.5)\n",
      "Requirement already satisfied: icecream>=2.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (2.1.3)\n",
      "Requirement already satisfied: spacy>=3.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (3.6.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (1.11.1)\n",
      "Requirement already satisfied: pygments>=2.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (2.15.1)\n",
      "Requirement already satisfied: networkx[default]>=2.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (3.1)\n",
      "Requirement already satisfied: graphviz>=0.13 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (0.20.1)\n",
      "Requirement already satisfied: executing>=0.3.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (1.2.0)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from networkx[default]>=2.6->pytextrank) (1.25.2)\n",
      "Requirement already satisfied: matplotlib>=3.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from networkx[default]>=2.6->pytextrank) (3.7.2)\n",
      "Requirement already satisfied: pandas>=1.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from networkx[default]>=2.6->pytextrank) (2.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.4.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.0.9)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (8.1.11)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (23.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (0.10.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.31.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
      "Requirement already satisfied: six in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (10.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (2.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0->pytextrank) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0->pytextrank) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.3)\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 955, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno -3] Temporary failure in name resolution\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 790, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request\n",
      "    raise new_e\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1092, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connection.py\", line 611, in connect\n",
      "    self.sock = sock = self._new_conn()\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connection.py\", line 210, in _new_conn\n",
      "    raise NameResolutionError(self.host, self, e) from e\n",
      "urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x7fdb42a90f10>: Failed to resolve 'raw.githubusercontent.com' ([Errno -3] Temporary failure in name resolution)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 844, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdb42a90f10>: Failed to resolve 'raw.githubusercontent.com' ([Errno -3] Temporary failure in name resolution)\"))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/spacy/cli/_util.py\", line 92, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/typer/core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/typer/core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/typer/main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/spacy/cli/download.py\", line 36, in download_cli\n",
      "    download(model, direct, sdist, *ctx.args)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/spacy/cli/download.py\", line 70, in download\n",
      "    compatibility = get_compatibility()\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/spacy/cli/download.py\", line 94, in get_compatibility\n",
      "    r = requests.get(about.__compatibility__)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fdb42a90f10>: Failed to resolve 'raw.githubusercontent.com' ([Errno -3] Temporary failure in name resolution)\"))\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pytextrank\n",
    "!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c55ce4-ab7e-48a6-abbd-d5ce385630ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0653ba11-2b67-463b-a73d-8efa4c69b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "from rake_nltk import Rake\n",
    "\n",
    "import yake\n",
    "\n",
    "import spacy, pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2654b878-7d5e-48ca-89ed-357cfa13393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, language, max_tokens):\n",
    "        self.language = language\n",
    "        self.max_tokens = max_tokens\n",
    "        pass\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        raise NotImplementedError(\"extract_terms method must be implemented in subclass\")\n",
    "\n",
    "    def extract_terms_with_span(self, text):\n",
    "        terms = self.extract_terms(text)\n",
    "        terms_with_span = self.find_term_span(text, terms)\n",
    "        return terms_with_span\n",
    "\n",
    "    def extract_terms_without_overlaps(self, text):\n",
    "        terms_with_span = self.extract_terms_with_span(text)\n",
    "        terms_without_overlaps = self.rmv_overlaps(terms_with_span)\n",
    "        return terms_without_overlaps\n",
    "\n",
    "    def postprocess_terms(self, terms):\n",
    "        new_terms = self.rmv_meaningless(terms)\n",
    "        new_terms = self.rmv_stopwords(new_terms)\n",
    "        new_terms = self.rmv_stopwords(new_terms)\n",
    "        new_terms = self.rmv_nonalnum_characters(new_terms)\n",
    "        new_terms = self.rmv_nonalnum_characters(new_terms)\n",
    "        new_terms = self.rmv_meaningless(new_terms)\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_meaningless(self, terms):\n",
    "        self.stopwords = nltk.corpus.stopwords.words(self.language)\n",
    "        new_terms = [t for t in terms if (t[0] not in self.stopwords and len(t[0]) >= 2 and not t[0].isdigit())]\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_stopwords(self, terms):\n",
    "        new_terms = terms\n",
    "        self.stopwords = nltk.corpus.stopwords.words(self.language)\n",
    "        for word in self.stopwords:\n",
    "            new_terms = [(t[0][(len(word)+1):], t[1] + len(word) + 1, t[2], t[3], t[4]) if (t[0].lower().startswith(word.lower() + \" \")) else t for t in new_terms]\n",
    "            new_terms = [(t[0][:-(len(word)+1)], t[1], t[2] - len(word) - 1, t[3], t[4]) if (t[0].lower().endswith(\" \" + word.lower())) else t for t in new_terms]\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_nonalnum_characters(self, terms):\n",
    "        new_terms = [(t[0][1:], t[1] + 1, t[2], t[3], t[4]) if not t[0][0].isalnum() else t for t in terms]\n",
    "        new_terms = [(t[0][:-1], t[1], t[2] - 1, t[3], t[4]) if not t[0][-1].isalnum() else t for t in new_terms]\n",
    "        return new_terms\n",
    "\n",
    "    @staticmethod\n",
    "    def find_term_span(text, terms):\n",
    "        spans = []\n",
    "        for t,score in terms:\n",
    "          term = re.escape(t)\n",
    "          patron = r'\\b' + term + r'\\b'\n",
    "          coincidencias = re.finditer(patron, text, re.IGNORECASE)\n",
    "          span = [(t, coincidencia.start(), coincidencia.end()-1, score) for coincidencia in coincidencias]\n",
    "          spans.extend(span)\n",
    "        return spans\n",
    "\n",
    "    @staticmethod\n",
    "    def rmv_overlaps(keywords):\n",
    "      ent = [kw[0] for kw in keywords]\n",
    "      pos = [kw[1:3] for kw in keywords]\n",
    "      score = [kw[3:] for kw in keywords]\n",
    "      updated_keywords = []\n",
    "      repeated_words = []\n",
    "      for i in range(len(ent)):\n",
    "        overlap = False\n",
    "        if pos[i] in repeated_words:\n",
    "          overlap = True\n",
    "        else:\n",
    "          repeated_words.append(pos[i])\n",
    "          for k in range(len(ent)):\n",
    "            if (((pos[i][0] >= pos[k][0]) and (pos[i][1] < pos[k][1])) or ((pos[i][0] > pos[k][0]) and (pos[i][1] <= pos[k][1]))):\n",
    "              overlap = True\n",
    "        if (not overlap):\n",
    "          kw = (ent[i],pos[i][0],pos[i][1]) + score[i]\n",
    "          updated_keywords.append(kw)\n",
    "      return updated_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3e947790-f7bd-479d-be58-1923941c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "        self.extractor = Rake(stopwords=self.stopwords, language=language)\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        self.extractor.extract_keywords_from_text(text)\n",
    "        terms = self.extractor.get_ranked_phrases_with_scores()\n",
    "        terms = [(kw,score) for score,kw in terms if (len(word_tokenize(kw, language=self.language)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b29313ad-4f4f-4aeb-832c-40c2aa71c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = yake.KeywordExtractor() #aqui habria que poner top=70 por ejemplo\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        keywords = self.extractor.extract_keywords(text)\n",
    "        terms = [(kw,score) for kw, score in keywords if (len(word_tokenize(kw)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f018e683-962a-4222-8a66-d6f99847d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = spacy.load(\"es_core_news_sm\")\n",
    "            self.extractor.add_pipe(\"textrank\")\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        doc = self.extractor(text)\n",
    "        terms = []\n",
    "        for phrase in doc._.phrases:\n",
    "          if (len(word_tokenize(phrase.text)) <= self.max_tokens):\n",
    "            terms.append((phrase.text, phrase.rank))\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5200cc68-2cfa-4fb1-95c1-a73291673d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    def __init__(self, extraction_methods=\"rake\", language=\"spanish\", max_tokens=3, join=False, postprocess=True): #maybe fer que max_tokens s hagi de posar mes tard??\n",
    "        self.extraction_methods = extraction_methods\n",
    "        self.extractors = self.initialize_keyword_extractors(language, max_tokens)\n",
    "        self.keywords = None\n",
    "        self.join = join\n",
    "        self.postprocess = postprocess\n",
    "\n",
    "    def __call__(self, text):\n",
    "        terms = self.extract_terms(text, self.join, self.postprocess)\n",
    "        return terms\n",
    "        \n",
    "    def initialize_keyword_extractors(self, language, max_tokens):\n",
    "        keyword_extractors = {} #esto esta hecho para mas de un extractor a la vez???\n",
    "        \n",
    "        if 'rake' in self.extraction_methods:\n",
    "            keyword_extractors[\"rake\"] = RakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'yake' in self.extraction_methods:\n",
    "            keyword_extractors[\"yake\"] = YakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'textrank' in self.extraction_methods:\n",
    "            keyword_extractors[\"textrank\"] = TextRankExtractor(language, max_tokens)\n",
    "\n",
    "        if not keyword_extractors:\n",
    "            raise ValueError(\"No extraction method called {}\".format(self.extraction_methods))\n",
    "        \n",
    "        return keyword_extractors\n",
    "\n",
    "    def extract_terms(self, text, join, postprocess):\n",
    "        try:\n",
    "            all_terms = []\n",
    "            for key, extractor in self.extractors.items():\n",
    "                terms = extractor.extract_terms_without_overlaps(text)\n",
    "                terms = [term + (key,) for term in terms]\n",
    "                all_terms.extend(terms)\n",
    "            if join:\n",
    "                all_terms = list(self.extractors.values())[0].rmv_overlaps(all_terms)\n",
    "            if postprocess:\n",
    "                all_terms = list(self.extractors.values())[0].postprocess_terms(all_terms)\n",
    "            self.keywords = [Keyword(text=i[0], method=i[4], ini=i[1], fin=i[2], score=i[3]) for i in all_terms]\n",
    "        except:\n",
    "            raise AttributeError(\"A list of extractors must be provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "61ae49e0-7615-4a76-b2f8-19b9f3eb9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword:\n",
    "    def __init__(self, text, method, ini, fin, score):\n",
    "        self.text = text\n",
    "        self.method = method\n",
    "        self.score = score\n",
    "        self.span = [ini, fin]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A method to return a string representation of the Keyword object.\n",
    "\n",
    "        Parameters:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - A string representing the Keyword object.\n",
    "        \"\"\"\n",
    "        return f\"<Keyword(text='{self.text}', method='{self.method}', score='{self.score}', span='{self.span}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "90cd4fc0-a22b-4af1-a3c4-28366c48f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\",\"yake\"],postprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5022439-f3c0-466f-965a-8aa47c0336a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RakeExtractor at 0x7fde096d6290>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(extractor.extractors.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "69df16e5-01d6-48fb-9e79-7446c051ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(\"La importancia vital de lavarse con jabón las manos antes de operar es esencial que que\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "207dda7a-c778-4992-ab3b-06cae37de34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('importancia vital', 'rake'), ('operar', 'rake'), ('manos', 'rake'), ('lavarse', 'rake'), ('jabón', 'rake'), ('esencial', 'rake'), ('lavarse con jabón', 'yake'), ('jabón', 'yake'), ('jabón las manos', 'yake'), ('manos', 'yake'), ('importancia vital', 'yake'), ('vital de lavarse', 'yake'), ('operar', 'yake'), ('operar es esencial', 'yake'), ('jabón', 'textrank'), ('importancia vital', 'textrank'), ('manos', 'textrank')]\n"
     ]
    }
   ],
   "source": [
    "a = [(kw.text,kw.method) for kw in extractor.keywords]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b0c30-5f8a-4647-98f6-eace29e7d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estaria be fer el post processing abans de rmv_overlaps perq aixi les que passen a ser insignificants despres del postprocessing les eliminem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8397a914-c7a7-47b4-8184-472a0cd32a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando TextRank y un máximo de 5 tokens: \n",
      " None\n",
      "\n",
      "Usando Rake y Yake juntos y un máximo de 2 tokens: \n",
      " None\n",
      "\n",
      "Usando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción completa directa\n",
    "text = \"Un varón de 32 años acude al Servicio de Urgencias por disminución reciente de visión en OD coincidiendo con la aparición de una lesión parduzca en dicho ojo. Entre los antecedentes oftalmológicos destaca un traumatismo penetrante en OD tres años antes que fue suturado en nuestro centro. A la exploración presenta una agudeza visual de 0,1 que mejora a 0,5 con estenopéico.\"\n",
    "\n",
    "extractor1 = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=5)\n",
    "print(\"Usando TextRank y un máximo de 5 tokens: \\n\",extractor1(text))\n",
    "\n",
    "extractor2 = TermExtractor(extraction_methods=[\"rake\", \"yake\"], language=\"spanish\", max_tokens=2)\n",
    "print(\"\\nUsando Rake y Yake juntos y un máximo de 2 tokens: \\n\",extractor2(text))\n",
    "\n",
    "extractor3 = TermExtractor(extraction_methods=[\"rake\", \"yake\", \"textrank\"], language=\"spanish\", max_tokens=3)\n",
    "print(\"\\nUsando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \\n\",extractor3(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9d95c8-a49f-4823-bb89-dcdb8ef6aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dicho ojo', 148, 156), ('disminución reciente', 55, 74), ('OD', 89, 90), ('OD', 234, 235), ('estenopéico', 362, 372), ('visión', 79, 84), ('una lesión parduzca', 125, 143), ('Servicio de Urgencias', 29, 49), ('un traumatismo penetrante', 205, 229), ('nuestro centro', 273, 286), ('una agudeza visual', 315, 332), ('la aparición', 109, 120), ('32 años', 12, 18), ('tres años', 237, 245), ('los antecedentes oftalmológicos', 165, 195), ('Un varón', 0, 7), ('A la exploración', 289, 304), ('que', 253, 255), ('que', 341, 343)]\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción por pasos: vemos que da lo mismo que el de antes\n",
    "extractor = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=3)\n",
    "terms = extractor.extractors[\"textrank\"].extract_terms(text)\n",
    "terms_with_span = extractor.extractors[\"textrank\"].find_term_span(text,terms)\n",
    "terms_without_overlaps = extractor.extractors[\"textrank\"].rmv_overlaps(terms_with_span)\n",
    "print(terms_without_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b02f74d-aa21-468f-bf80-a2ba888154da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348c6565-7bfd-4065-92a4-5d54b191f04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['importancia vital', 'operar', 'manos', 'lavarse', 'jabón', 'indispensable']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].extract_terms(\"La importancia vital de lavarse con jabón las manos antes de operar es indispensable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c4e94-b6d8-4b86-9c60-2fb1f3382f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_terminos = TermExtractor(sadasdasdsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4f1f2-3605-4daa-8e94-8c8859cd6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms = extractor_terminos(\"textoasdasdasdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4089de-7798-443f-a475-fd0f37b6a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms.zº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33845a0-6647-4236-b667-89b62d67c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e722f2-66b9-4577-adda-5910044dcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "general_path = os.getcwd().split(\"BioTermCategorizer\")[0]+\"BioTermCategorizer/\"\n",
    "sys.path.append(general_path+'biotermcategorizer/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d679dac1-4248-4c69-9fd4-af97d84e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TermExtractor import TermExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d9605d-1c9b-44db-b767-5c67541f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "199201ba-58db-4dd2-85f2-7da7fbc1ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53702ec3-6063-4897-a4cb-e16c88da3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample texts\n",
    "text1 = \"Paciente varón de 35 años con tumoración en polo superior de teste derecho hallada de manera casual durante una autoexploración, motivo por el cual acude a consulta de urología donde se realiza exploración física, apreciando masa de 1cm aproximado de diámetro dependiente de epidídimo, y ecografía testicular, que se informa como lesión nodular sólida en cabeza de epidídimo derecho. Se realiza RMN. Confirmando masa nodular, siendo el tumor adenomatoide de epidídimo la primera posibilidad diagnóstica. Se decide, en los dos casos, resección quirúrgica de tumoración nodular en cola epidídimo derecho, sin realización de orquiectomía posterior. En ambos casos se realizó examen anátomopatológico de la pieza quirúrgica. Hallazgos histológicos macroscópicos: formación nodular de 1,5 cms (caso1) y 1,2 cms (caso 2) de consistencia firme, coloración blanquecina y bien delimitada. Microscópicamente se observa proliferación tumoral constituida por estructuras tubulares en las que la celularidad muestra núcleos redondeados y elongados sin atipia citológica y que ocasionalmente muestra citoplasmas vacuolados, todo ello compatible con tumor adenomatoide de epidídimo.\"\n",
    "text2 = \"Dos recién nacidos, varón y hembra de una misma madre y fallecidos a los 10 y 45 minutos de vida respectivamente a los que se les realizó examen necrópsico. El primero de los cadáveres, correspondiente a la hembra, fue remitido con el juicio clínico de insuficiencia respiratoria grave con sospecha de Síndrome de Potter con la constatación de oligoamnios severo; nació mediante cesárea urgente por presentación de nalgas y el test de Apgar fue 1/3/7; minutos más tarde falleció. El examen externo permitió observar una tonalidad subcianótica, facies triangular con hendiduras parpebrales mongoloides, micrognatia, raiz nasal ancha y occipucio prominente. El abdomen, globuloso, duro y ligeramente abollonado permitía la palpación de dos grandes masas ocupando ambas fosas renales y hemiabdomenes. A la apertura de cavidades destacaba la presencia de dos grandes masas renales de 10 x 8 x 5,5 cm y 12 x 8 x 6 cm con pesos de 190 y 235 gr respectivamente. Si bien se podía discernir la silueta renal, la superficie, abollonada, presentaba numerosas formaciones quísticas de contenido seroso; al corte dichos quistes mostraban un tamaño heterogéneo siendo mayores los situados a nivel cortical, dando al riñón un aspecto de esponja. Los pulmones derecho e izquierdo pesaban 17 y 15 gr (peso habitual del conjunto de 49 gr) mostrando una tonalidad rojiza uniforme; ambos se encontraban comprimidos como consecuencia de la elevación diafragmática condicionada por el gran tamaño de los riñones. El resto de los órganos no mostraba alteraciones macroscópicas significativas salvo las alteraciones posicionales derivadas de la compresión renal. En el segundo de los cadáveres, el correspondiente al varón, se observaron cambios morfológicos similares si bien el tamaño exhibido por los riñones era aún mayor, con pesos de 300 y 310 gr. El resto de las vísceras abdominales estaban comprimidas contra el diafragma. En ambos casos se realizó un estudio histológico detallado, centrado especialmente en los riñones en los que se demostraron múltiples quistes de distintos tamaños con morfología sacular a nivel cortical. Dichos quistes ocupaban la mayor parte del parénquima corticomedular si bien las zonas conservadas no mostraban alteraciones significativas salvo inmadurez focal. Dichos quistes estaban tapizados por un epitelio simple que variaba desde plano o cúbico. Los quistes medulares, de menor tamaño y más redondeados estaban tapizados por un epitelio de predominio cúbico. Después de las renales, las alteraciones más llamativas se encontraban en el hígado donde se observaron proliferación y dilatación, incluso quística, de los ductos biliares a nivel de los espacios porta. Con tales hallazgos se emitió en ambos casos el diagnóstico de enfermedad poliquística renal autosómica recesiva infantil.\"\n",
    "text3 = \"Paciente de 64 años, alérgico a penicilina y con recambio valvular aórtico por endocarditis que consultó por aparición de masa peneana de crecimiento progresivo en las últimas semanas. A la exploración física destacaba una formación excrecente y abigarrada en glande, que deformaba meato, con áreas ulceradas cubiertas de fibrina. Se palpaban adenopatías fijas y duras en ambas regiones inguinales. La radiografía de tórax y el TAC abdomino-pélvico confirmaron la presencia de adenopatías pulmonares e inguinales de gran tamaño. Con el diagnóstico de neoplasia de pene, se practicó penectomía parcial con margen de seguridad. La anatomía patológica demostró que se trataba de un sarcoma pleomórfico de pene con diferenciación osteosarcomatosa y márgenes libres de afectación. Se decidió tratamiento con dos líneas de quimioterapia consistente en adriamicina e ifosfamida pero no hubo respuesta. Ingresó de nuevo con recidiva local sangrante de gran tamaño y crecimiento rápido que provocaba obstrucción de meato con insuficiencia renal aguda. Se colocó sonda de cistostomía y se instauró tratamiento con sueroterapia, mejorando la función renal, pero con empeoramiento progresivo del estado general hasta que falleció a los 6 meses del diagnóstico.\"\n",
    "text4 = \"Mujer de 28 años sin antecedentes de interés que consultó por síndrome miccional con polaquiuria de predominio diurno y cierto grado de urgencia sin escapes urinario. El urocultivo resultó negativo por lo que se indicó tratamiento con anticolinérgicos. Ante la falta de respuesta al tratamiento, se realizó cistografía que fué normal y ecografía renovesical en la que se apreciaban imágenes quísticas parapiélicas, algunas de ellas con tabiques internos y vejiga sin lesiones. Con el fin de precisar la naturaleza de dichos quistes se solicitó TAC-abdominal, que informaba de gran quiste parapiélico en riñón derecho sin repercusión sobre la vía y una masa hipodensa suprarrenal derecha. La resonancia magnética demostró normalidad de la glándula suprarrenal y una lesión quística lobulada conteniendo numerosos septos en su interior que rodeaba al riñón derecho; en la celda renal izquierda existía una lesión de características similares pero de menor tamaño. Los hallazgos eran compatibles con linfangioma renal bilateral. Tras tres años de seguimiento la paciente continua con leve síntomatologia miccional en tratamiento, pero no ha presentado síntomas derivados de su lesión renal.\"\n",
    "text5 = \"Varón de 68 años, con antecedentes de hemorragia digestiva alta por aspirina y accidente isquémico transitorio a tratamiento crónico con trifusal (300 mg cada 12 horas), que acudió al Servicio de Urgencias del Hospital San Agustín (Avilés, Asturias), en mayo de 2006, por dolor en hemiabdomen izquierdo, intenso, continuo, de instauración súbita y acompañado de cortejo vegetativo. A la exploración presentaba una tensión arterial de 210/120 mm Hg, una frecuencia cardíaca de 80 por minuto, y dolor en fosa ilíaca izquierda, acentuado con la palpación. El hemograma (hemoglobina: 13 g/dL, plaquetas: 249.000), el estudio de coagulación, la bioquímica elemental de sangre, el sistemático de orina, el electrocardiograma y la radiografía simple de tórax eran normales. En la tomografía computarizada de abdomen se objetivó un extenso hematoma, de 12 cm de diámetro máximo, en la celda renal izquierda, sin líquido libre intraperitoneal; la suprarrenal izquierda quedaba englobada y no se podía identificar, y la derecha no presentaba alteraciones. La HTA no se llegó a controlar en Urgencias, a pesar del tratamiento con analgésicos, con antagonistas del calcio y con inhibidores de la enzima convertidora de la angiotensina II, por lo que el paciente, que mantenía cifras tensionales de 240/160 mm Hg, pasó a la Unidad de Cuidados Intensivos, para tratamiento intravenoso con nitroprusiato y labetalol. En las 24 horas siguientes se yuguló la crisis hipertensiva, y se comprobó que la hemoglobina y el hematocrito permanecían estables. Con la sospecha diagnóstica de rotura no traumática de un feocromocitoma pre-existente, se determinaron metanefrinas plasmáticas, que fueron normales, y catecolaminas y metanefrinas urinarias. En la orina de 24 horas del día siguiente al ingreso se obtuvieron los siguientes resultados: adrenalina: 65,1 mcg (valores normales -VN: 1,7-22,5), noradrenalina: 151,1 mcg (VN: 12,1-85,5), metanefrina: 853,5 mcg (VN: 74-297) y normetanefrina: 1396,6 mcg (VN: 105-354). A los 10 días, todavía ingresado el paciente, las cifras urinarias se habían normalizado por completo de modo espontáneo. Respecto al hematoma, en julio de 2006 no se había reabsorbido y persistía una imagen pseudoquística en la zona suprarrenal izquierda. En septiembre de 2006 se practicó una suprarrenalectomía unilateral, y el estudio histológico mostró una masa encapsulada de 6 x 5 cm, con necrosis hemorrágica extensa y algunas células corticales sin atipias.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f00dff96-03fd-4195-902f-cdaec9d63a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "141\n",
      "67\n",
      "52\n",
      "152\n",
      "CPU times: user 160 ms, sys: 1.38 ms, total: 161 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2656ba7-6002-4586-9dec-622fc334dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "35\n",
      "19\n",
      "20\n",
      "14\n",
      "CPU times: user 545 ms, sys: 5.49 ms, total: 551 ms\n",
      "Wall time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"yake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e47ebf00-f778-4157-8a13-0729c269cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "95\n",
      "52\n",
      "53\n",
      "85\n",
      "CPU times: user 1.04 s, sys: 88.3 ms, total: 1.12 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"textrank\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f99d9ad2-cede-4005-bb85-9829e54a1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "173\n",
      "84\n",
      "76\n",
      "164\n",
      "CPU times: user 1.76 s, sys: 97.7 ms, total: 1.86 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\",\"yake\",\"textrank\"], join=True)\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31d9bc3-e2fd-4561-b6ce-a35c13e58476",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e3947b-b249-41b3-aba4-26a27e171ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Keyword(text='realizó examen anátomopatológico', method='rake', score='9.0', spans = '[664, 695]')>\n"
     ]
    }
   ],
   "source": [
    "a = extractor.keywords[0]\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
