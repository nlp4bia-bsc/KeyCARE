{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c55ce4-ab7e-48a6-abbd-d5ce385630ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0653ba11-2b67-463b-a73d-8efa4c69b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "from rake_nltk import Rake\n",
    "\n",
    "import yake\n",
    "\n",
    "import spacy, pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2654b878-7d5e-48ca-89ed-357cfa13393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, language, max_tokens):\n",
    "        self.language = language\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        raise NotImplementedError(\"extract_terms method must be implemented in subclass\")\n",
    "\n",
    "    def extract_terms_with_span(self, text):\n",
    "        terms = self.extract_terms(text)\n",
    "        terms_with_span = self.find_term_span(text, terms)\n",
    "        return terms_with_span\n",
    "\n",
    "    #if some of the kwargs do not exist, they will simply be ignored\n",
    "    def extract_terms_without_overlaps(self, text, kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        terms_with_span = self.extract_terms_with_span(text)\n",
    "        terms_without_overlaps = self.rmv_overlaps(terms_with_span)\n",
    "        return terms_without_overlaps\n",
    "\n",
    "    def postprocess_terms(self, terms):\n",
    "        new_terms = self.rmv_meaningless(terms)\n",
    "        new_terms = self.rmv_stopwords(new_terms)\n",
    "        new_terms = self.rmv_stopwords(new_terms)\n",
    "        new_terms = self.rmv_nonalnum_characters(new_terms)\n",
    "        new_terms = self.rmv_nonalnum_characters(new_terms)\n",
    "        new_terms = self.rmv_meaningless(new_terms)\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_meaningless(self, terms):\n",
    "        self.stopwords = nltk.corpus.stopwords.words(self.language)\n",
    "        new_terms = [t for t in terms if (t[0] not in self.stopwords and len(t[0]) >= 2 and not t[0].isdigit())]\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_stopwords(self, terms):\n",
    "        new_terms = terms\n",
    "        self.stopwords = nltk.corpus.stopwords.words(self.language)\n",
    "        for word in self.stopwords:\n",
    "            new_terms = [(t[0][(len(word)+1):], t[1] + len(word) + 1, t[2], t[3], t[4]) if (t[0].lower().startswith(word.lower() + \" \")) else t for t in new_terms]\n",
    "            new_terms = [(t[0][:-(len(word)+1)], t[1], t[2] - len(word) - 1, t[3], t[4]) if (t[0].lower().endswith(\" \" + word.lower())) else t for t in new_terms]\n",
    "        return new_terms\n",
    "\n",
    "    def rmv_nonalnum_characters(self, terms):\n",
    "        new_terms = [(t[0][1:], t[1] + 1, t[2], t[3], t[4]) if not t[0][0].isalnum() else t for t in terms]\n",
    "        new_terms = [(t[0][:-1], t[1], t[2] - 1, t[3], t[4]) if not t[0][-1].isalnum() else t for t in new_terms]\n",
    "        return new_terms\n",
    "\n",
    "    @staticmethod\n",
    "    def find_term_span(text, terms):\n",
    "        spans = []\n",
    "        for t,score in terms:\n",
    "          term = re.escape(t)\n",
    "          patron = r'\\b' + term + r'\\b'\n",
    "          coincidencias = re.finditer(patron, text, re.IGNORECASE)\n",
    "          span = [(t, coincidencia.start(), coincidencia.end()-1, score) for coincidencia in coincidencias]\n",
    "          spans.extend(span)\n",
    "        return spans\n",
    "\n",
    "    @staticmethod\n",
    "    def rmv_overlaps(keywords):\n",
    "      ent = [kw[0] for kw in keywords]\n",
    "      pos = [kw[1:3] for kw in keywords]\n",
    "      score = [kw[3:] for kw in keywords]\n",
    "      updated_keywords = []\n",
    "      repeated_words = []\n",
    "      for i in range(len(ent)):\n",
    "        overlap = False\n",
    "        if pos[i] in repeated_words:\n",
    "          overlap = True\n",
    "        else:\n",
    "          repeated_words.append(pos[i])\n",
    "          for k in range(len(ent)):\n",
    "            if (((pos[i][0] >= pos[k][0]) and (pos[i][1] < pos[k][1])) or ((pos[i][0] > pos[k][0]) and (pos[i][1] <= pos[k][1]))):\n",
    "              overlap = True\n",
    "        if (not overlap):\n",
    "          kw = (ent[i],pos[i][0],pos[i][1]) + score[i]\n",
    "          updated_keywords.append(kw)\n",
    "      return updated_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e947790-f7bd-479d-be58-1923941c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "        self.extractor = Rake(stopwords=self.stopwords, language=language)\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        filtered_kwargs = {key: value for key, value in self.kwargs.items() if key in self.extractor.extract_keywords_from_text.__code__.co_varnames}\n",
    "        self.extractor.extract_keywords_from_text(text, **filtered_kwargs)\n",
    "        terms = self.extractor.get_ranked_phrases_with_scores()\n",
    "        terms = [(kw,score) for score,kw in terms if (len(word_tokenize(kw, language=self.language)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29313ad-4f4f-4aeb-832c-40c2aa71c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = yake.KeywordExtractor() #aqui habria que poner top=70 por ejemplo\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        filtered_kwargs = {key: value for key, value in self.kwargs.items() if key in self.extractor.extract_keywords.__code__.co_varnames}\n",
    "        keywords = self.extractor.extract_keywords(text, **filtered_kwargs)\n",
    "        terms = [(kw,score) for kw, score in keywords if (len(word_tokenize(kw)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f018e683-962a-4222-8a66-d6f99847d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = spacy.load(\"es_core_news_sm\")\n",
    "            self.extractor.add_pipe(\"textrank\")\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        filtered_kwargs = {key: value for key, value in self.kwargs.items() if key in self.extractor.__call__.__code__.co_varnames}\n",
    "        doc = self.extractor(text, **filtered_kwargs)\n",
    "        terms = []\n",
    "        for phrase in doc._.phrases:\n",
    "          if (len(word_tokenize(phrase.text)) <= self.max_tokens):\n",
    "            terms.append((phrase.text, phrase.rank))\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a869e67d-abbe-4b9a-91b5-018167f1ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098081a4-7481-42f2-a79b-1d89c531b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyBertExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        sentence_model = self.initialize_sentence_model()\n",
    "        self.extractor = KeyBERT(model=sentence_model)\n",
    "\n",
    "    def initialize_sentence_model(self):\n",
    "        path = '/mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_parents_1epoch'\n",
    "        sentence_model = SentenceTransformer(path)\n",
    "        return sentence_model\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        filtered_kwargs = {key: value for key, value in self.kwargs.items() if key in self.extractor.extract_keywords.__code__.co_varnames}\n",
    "        specified_params = ['keyphrase_ngram_range', 'stop_words', 'use_mmr']\n",
    "        for param in specified_params:\n",
    "            if param in filtered_kwargs:\n",
    "                del filtered_kwargs[param]\n",
    "                print(\"Warning. The parameter \" + param + \" can not be changed from its given value.\")\n",
    "        keywords_all = list()\n",
    "        for i in range(self.max_tokens):\n",
    "            keywords_i = self.extractor.extract_keywords(text, keyphrase_ngram_range=(1, i+1), stop_words=None, use_mmr=True, **filtered_kwargs)\n",
    "            keywords_all.extend(keywords_i)\n",
    "        terms = sorted(keywords_all, key=lambda x: x[1], reverse=True)\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5200cc68-2cfa-4fb1-95c1-a73291673d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    def __init__(self,\n",
    "                 extraction_methods=[\"textrank\"], \n",
    "                 categorizer_method=\"setfit\", \n",
    "                 language=\"spanish\", \n",
    "                 max_tokens=3, \n",
    "                 join=False, \n",
    "                 postprocess=True,\n",
    "                 n=1, \n",
    "                 thr_setfit=0.5,\n",
    "                 thr_transformers=-1,\n",
    "                 n_clusters=None,\n",
    "                 categorizer_model_path=None,\n",
    "                 output_path=\"./trained_model\", \n",
    "                 clustering_model=\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",\n",
    "                 classifier_model=\"/mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_noparents_1epoch\",\n",
    "                 **kwargs,\n",
    "                ):\n",
    "        self.extraction_methods = extraction_methods\n",
    "        self.extractors = self.initialize_keyword_extractors(language, max_tokens)\n",
    "        self.categorizer_method = categorizer_method\n",
    "        self.categorizer = self.initialize_categorizers(n, thr_setfit, thr_transformers, n_clusters, categorizer_model_path, output_path, clustering_model, classifier_model)\n",
    "        self.join = join\n",
    "        self.postprocess = postprocess\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        self.extract_terms(text, self.join, self.postprocess)\n",
    "        self.categorize_terms()\n",
    "        \n",
    "    def initialize_keyword_extractors(self, language, max_tokens):\n",
    "        keyword_extractors = {}\n",
    "        \n",
    "        if 'rake' in self.extraction_methods:\n",
    "            keyword_extractors[\"rake\"] = RakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'yake' in self.extraction_methods:\n",
    "            keyword_extractors[\"yake\"] = YakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'textrank' in self.extraction_methods:\n",
    "            keyword_extractors[\"textrank\"] = TextRankExtractor(language, max_tokens)\n",
    "\n",
    "        if 'keybert' in self.extraction_methods:\n",
    "            keyword_extractors[\"keybert\"] = KeyBertExtractor(language, max_tokens)\n",
    "\n",
    "        if not keyword_extractors:\n",
    "            raise ValueError(\"No extraction method called {}\".format(self.extraction_methods))\n",
    "        \n",
    "        return keyword_extractors\n",
    "\n",
    "    def extract_terms(self, text, join, postprocess):\n",
    "        try:\n",
    "            all_terms = []\n",
    "            for key, extractor in self.extractors.items():\n",
    "                terms = extractor.extract_terms_without_overlaps(text, kwargs=self.kwargs)\n",
    "                terms = [term + (key,) for term in terms]\n",
    "                all_terms.extend(terms)\n",
    "            if join:\n",
    "                all_terms = list(self.extractors.values())[0].rmv_overlaps(all_terms)\n",
    "            if postprocess:\n",
    "                all_terms = list(self.extractors.values())[0].postprocess_terms(all_terms)\n",
    "                all_terms = list(self.extractors.values())[0].rmv_overlaps(all_terms)\n",
    "            self.keywords = [Keyword(text=i[0], extraction_method=i[4], ini=i[1], fin=i[2], score=i[3]) for i in all_terms]\n",
    "        except:\n",
    "            raise AttributeError(\"A list of extractors must be provided\")\n",
    "\n",
    "    def initialize_categorizers(self, n, thr_setfit, thr_transformers, n_clusters, model_path, output_path, clustering_model, classifier_model):\n",
    "        if 'transformers' == self.categorizer_method:\n",
    "            categorizer = TransformersClassifier(n, thr_transformers, model_path, output_path, classifier_model)\n",
    "            \n",
    "        elif 'setfit' == self.categorizer_method:\n",
    "            categorizer = SetFitClassifier(n, thr_setfit, model_path, output_path, classifier_model)\n",
    "\n",
    "        elif 'clustering' == self.categorizer_method:\n",
    "            if n_clusters is None:\n",
    "                raise TypeError(\"TermExtractor.__init__() missing 1 required positional argument: 'n_clusters' when selecting the Clustering algorithm\")\n",
    "            categorizer = Clustering(n_clusters, model_path, output_path, clustering_model)\n",
    "        else:\n",
    "            raise ValueError(\"No categorizer method called {}\".format(self.categorizer_method))\n",
    "        return categorizer\n",
    "\n",
    "    def categorize_terms(self):\n",
    "        try:\n",
    "            if self.categorizer_method == 'clustering':\n",
    "                list_of_keywords = [kw.text for kw in self.keywords]\n",
    "                clusters = self.categorizer.predict_clusters(list_of_keywords)\n",
    "                for kw in self.keywords:\n",
    "                    kw.label = clusters[kw.text]\n",
    "                    kw.categorization_method = self.categorizer_method\n",
    "            else:\n",
    "                for kw in self.keywords:\n",
    "                    kw.label = self.categorizer.compute_predictions(kw.text)\n",
    "                    kw.categorization_method = self.categorizer_method\n",
    "        except:\n",
    "            raise AttributeError(\"A categorizer method must be provided\")\n",
    "\n",
    "    def train_classifier(self, trainX, trainY, testX, testY, mcm=False, classification_report=False, **kwargs):\n",
    "        self.categorizer.initialize_model_body(trainY)\n",
    "        metrics = self.categorizer.train_evaluate(trainX, trainY, testX, testY, mcm, classification_report, **kwargs)\n",
    "        print(metrics)\n",
    "\n",
    "    def train_clustering(self, trainX, **kwargs):\n",
    "        self.categorizer.train_clusters(trainX, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61ae49e0-7615-4a76-b2f8-19b9f3eb9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword:\n",
    "    def __init__(self, text, extraction_method, ini, fin, score):\n",
    "        self.text = text\n",
    "        self.extraction_method = extraction_method\n",
    "        self.score = score\n",
    "        self.span = [ini, fin]\n",
    "        self.categorization_method = None\n",
    "        self.label = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Keyword(text='{self.text}', span='{self.span}', extraction method='{self.extraction_method}', score='{self.score}', categorization method='{self.categorization_method}', class='{self.label}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83773cb-0c61-4f0b-834f-3a0350025de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2669b7-cd30-4d98-9061-0cd0b7ca3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5f9450-6a98-42ba-b11f-7d91f6b4f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorizer:\n",
    "    def __init__(self, n, threshold, model_path, output_path, classifier_model):\n",
    "        self.n = n\n",
    "        self.threshold = threshold\n",
    "        self.labels = ['ACTIVIDAD', 'COMUNIDAD', 'DEPARTAMENTO', 'ENFERMEDAD', 'FAC_GEN','FAC_NOM', 'FARMACO', 'GEO_GEN', 'GEO_NOM', 'GPE_GEN', 'GPE_NOM','HUMAN', 'IDIOMA', 'MORFOLOGIA_NEOPLASIA', 'NO_CATEGORY','PROCEDIMIENTO', 'PROFESION', 'SINTOMA', 'SITUACION_LABORAL','SPECIES', 'TRANSPORTE']\n",
    "        self.output_path = output_path\n",
    "        self.classifier_model = classifier_model\n",
    "        self.model = self.initialize_pretrained_model(model_path)\n",
    "\n",
    "    def evaluate_model(self, y_pred, y_test, mcm, classification_report):\n",
    "        if classification_report:\n",
    "            self.classification_report(y_pred, y_test)\n",
    "        if mcm:\n",
    "            self.mcm_heatmap(y_pred, y_test)\n",
    "        return self.compute_metrics(y_pred, y_test)\n",
    "\n",
    "    def lambda_evaluate_model(self, mcm, classification_report):\n",
    "        return lambda y_pred, y_test: self.evaluate_model(y_pred, y_test, mcm, classification_report)\n",
    "\n",
    "    def compute_metrics(self, y_pred, y_test):\n",
    "        multilabel_f1_metric = evaluate.load(\"f1\", \"multilabel\")\n",
    "        multilabel_accuracy_metric = evaluate.load(\"accuracy\", \"multilabel\")\n",
    "        f1 = multilabel_f1_metric.compute(predictions=y_pred, references=y_test, average=\"micro\")[\"f1\"]\n",
    "        accuracy = multilabel_accuracy_metric.compute(predictions=y_pred, references=y_test)[\"accuracy\"]\n",
    "\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_test = np.array(y_test)\n",
    "        \n",
    "        no_label_samples = []\n",
    "        for idx, pred in enumerate(y_pred):\n",
    "            if np.all(pred == 0):\n",
    "                true_labels = [self.labels[i] for i, value in enumerate(y_test[idx]) if value == 1]\n",
    "                no_label_samples.extend(true_labels)\n",
    "    \n",
    "        label_counts = Counter(no_label_samples)\n",
    "        label_counts_dict = dict(label_counts)\n",
    "        return {\"f1\": f1, \"accuracy\": accuracy, \"Classes with no given label\": label_counts_dict}\n",
    "\n",
    "    def classification_report(self, y_pred, y_test):\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        print(classification_report(y_test, y_pred, target_names=self.labels))\n",
    "\n",
    "    def mcm_heatmap(self, y_pred, y_test):\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_test = np.array(y_test)\n",
    "        \n",
    "        samples_with_predictions = np.any(y_pred, axis=1)\n",
    "    \n",
    "        # Filter y_pred and y_test to include only samples with predictions\n",
    "        y_pred_with_predictions = y_pred[samples_with_predictions]\n",
    "        y_test_with_predictions = y_test[samples_with_predictions]\n",
    "    \n",
    "        # Initialize the confusion matrix\n",
    "        confusion_matrix_multi = confusion_matrix(y_test_with_predictions.argmax(axis=1), y_pred_with_predictions.argmax(axis=1), labels=np.arange(len(self.labels)))\n",
    "    \n",
    "        # Create a heatmap-style visualization\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(confusion_matrix_multi, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        plt.title(\"Multi-Label Confusion Matrix\")\n",
    "        plt.colorbar()\n",
    "    \n",
    "        # Label the axes\n",
    "        tick_marks = np.arange(len(self.labels))\n",
    "        plt.xticks(tick_marks, self.labels, rotation=45)\n",
    "        plt.yticks(tick_marks, self.labels)\n",
    "    \n",
    "        # Display the values inside the cells\n",
    "        for i in range(len(self.labels)):\n",
    "            for j in range(len(self.labels)):\n",
    "                plt.text(j, i, str(confusion_matrix_multi[i, j]), ha=\"center\", va=\"center\", color=\"white\" if confusion_matrix_multi[i, j] > confusion_matrix_multi.max() / 2 else \"black\")\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60137a3-16f5-4d36-bf54-9a602af437e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafc52e7-c793-4cb6-91ea-7c7646ca3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4015fb04-549f-418c-b3cf-edcd5d538b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFitClassifier(Categorizer):\n",
    "    def __init__(self, n, threshold, model_path, output_path, classifier_model):\n",
    "        super().__init__(n, threshold, model_path, output_path, classifier_model)\n",
    "    \n",
    "    def initialize_pretrained_model(self, model_path):\n",
    "        if model_path is None:\n",
    "            path = '/mnt/c/Users/Sergi/Desktop/BSC/modelos_entrenados/SetFit/noparents_sp'\n",
    "            model = SetFitModel.from_pretrained(path)\n",
    "        else:\n",
    "            model = SetFitModel.from_pretrained(model_path)\n",
    "        return model\n",
    "\n",
    "    def compute_predictions(self, mention):\n",
    "        embeddings = self.model.model_body.encode([mention], normalize_embeddings=self.model.normalize_embeddings, convert_to_tensor=True)\n",
    "        predicts = self.model.model_head.predict_proba(embeddings)\n",
    "        predscores = {self.labels[i]: arr[:,1].tolist()[0] for i, arr in enumerate(predicts)}\n",
    "        top_n_labels = sorted(predscores, key=predscores.get, reverse=True)[:self.n]\n",
    "        filtered_labels = [label for label in top_n_labels if predscores[label] > self.threshold]\n",
    "        return filtered_labels\n",
    "\n",
    "    def initialize_model_body(self, trainY):\n",
    "        self.model = SetFitModel.from_pretrained(self.classifier_model, multi_target_strategy=\"multi-output\")\n",
    "\n",
    "    def train_evaluate(self, trainX, trainY, testX, testY, mcm, classification_report, **kwargs):\n",
    "        train_dataset, test_dataset = self.prepare_data(trainX, trainY, testX, testY)\n",
    "        evaluate_with_params = self.lambda_evaluate_model(mcm, classification_report)\n",
    "        specified_params = ['metric', 'num_iterations']\n",
    "        for param in specified_params:\n",
    "            if param in kwargs:\n",
    "                del kwargs[param]\n",
    "                print(\"Warning. The parameter \" + param + \" can not be changed from its given value.\")\n",
    "        trainer = SetFitTrainer(model=self.model, train_dataset=train_dataset, eval_dataset=test_dataset, metric=evaluate_with_params, num_iterations=5, **kwargs)\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        self.model.save_pretrained(self.output_path)\n",
    "        return metrics\n",
    "\n",
    "    def prepare_data(self, trainX, trainY, testX, testY):\n",
    "        trainY = [{i} for i in trainY]\n",
    "        testY = [{i} for i in testY]\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        mlb.fit_transform(trainY)\n",
    "        self.labels = [i for i in mlb.classes_]\n",
    "        train_dataset = Dataset.from_dict({\"text\": trainX, \"label\": mlb.fit_transform(trainY)})\n",
    "        test_dataset = Dataset.from_dict({\"text\": testX, \"label\": mlb.transform(testY)})\n",
    "        return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d45e560-3fbd-4112-afc2-2f9e6e6348fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d626d345-d66c-4fb3-8d32-490ee881c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersClassifier(Categorizer):\n",
    "    def __init__(self, n, threshold, model_path, output_path, classifier_model):\n",
    "        super().__init__(n, threshold, model_path, output_path, classifier_model)\n",
    "        \n",
    "    def initialize_pretrained_model(self, model_path):\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit([self.labels])\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.classifier_model)\n",
    "        if model_path is None:\n",
    "            path = '/mnt/c/Users/Sergi/Desktop/BSC/modelos_entrenados/Transformers/noparents_sp'        \n",
    "            model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        else:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        return model\n",
    "\n",
    "    def compute_predictions(self, mention):\n",
    "        tokenized_mention = self.tokenizer(mention, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**tokenized_mention)\n",
    "        logits = output.logits\n",
    "        predscores = {label: score for label, score in zip(self.labels, logits.tolist()[0])}\n",
    "        top_n_labels = sorted(predscores, key=predscores.get, reverse=True)[:self.n]\n",
    "        filtered_labels = [label for label in top_n_labels if predscores[label] > self.threshold]\n",
    "        return filtered_labels\n",
    "    \n",
    "    def initialize_model_body(self, trainY):\n",
    "        self.find_labels(trainY)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.classifier_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.classifier_model, num_labels=self.num_labels, problem_type=\"multi_label_classification\")\n",
    "\n",
    "    #Im doing it as if I received a dataframe with the samples in columns called \"text\" and \"label\"\n",
    "    def train_evaluate(self, trainX, trainY, testX, testY, mcm, classification_report, **kwargs):\n",
    "        train_dataset = self.prepare_data(trainX, trainY)\n",
    "        self.train(train_dataset, **kwargs)\n",
    "        test_dataset = self.prepare_data(testX, testY)\n",
    "        self.evaluate(test_dataset, mcm, classification_report)\n",
    "        self.model.save_pretrained(self.output_path)\n",
    "\n",
    "    def find_labels(self, trainY):\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        Y = [[i] for i in trainY]\n",
    "        self.mlb.fit(Y)\n",
    "        self.labels = [i for i in self.mlb.classes_]\n",
    "        self.num_labels = len(self.labels)\n",
    "    \n",
    "    def prepare_data(self, X, Y):\n",
    "        tokenized_data = self.tokenizer(X, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "        label_strings = [[i] for i in Y]\n",
    "        labels = self.mlb.transform(label_strings)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        tensordataset = TensorDataset(tokenized_data.input_ids, tokenized_data.attention_mask, labels)\n",
    "        return tensordataset\n",
    "\n",
    "    def train(self, train_dataset, **kwargs):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./output\",  # Output directory\n",
    "            num_train_epochs=3,     # Number of training epochs\n",
    "            per_device_train_batch_size=32,  # Batch size per device\n",
    "            evaluation_strategy=\"steps\",  # Evaluate every steps\n",
    "            save_steps=500,  # Save checkpoint every 500 steps\n",
    "            save_total_limit=2,  # Only keep the last 2 checkpoints\n",
    "            load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "        )\n",
    "        specified_params = ['data_collator', 'args','model']\n",
    "        for param in specified_params:\n",
    "            if param in kwargs:\n",
    "                del kwargs[param]\n",
    "                print(\"Warning. The parameter \" + param + \" can not be changed from its given value.\")\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=self.collate_fn,  # You can customize data collation if needed\n",
    "            train_dataset=train_dataset,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.trainer.train()\n",
    "\n",
    "    def evaluate(self, testset, mcm, classification_report):\n",
    "        results = self.trainer.predict(testset)\n",
    "        max_indices = np.argmax(results.predictions, axis=1)\n",
    "        preds = np.zeros_like(results.predictions)\n",
    "        preds[np.arange(len(max_indices)), max_indices] = 1\n",
    "        metrics = self.evaluate_model(preds, results.label_ids, mcm, classification_report)\n",
    "        print(metrics)\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        return {\n",
    "        'input_ids': torch.stack([item[0] for item in batch]),\n",
    "        'attention_mask': torch.stack([item[1] for item in batch]),\n",
    "        'labels': torch.stack([item[2] for item in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5558f027-8437-4b85-be3b-5be7f29fa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa830706-5ff2-41bc-b3d8-e390f22d0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering(Categorizer):\n",
    "    def __init__(self, n_clusters, model_path, output_path, clustering_model):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(clustering_model)\n",
    "        self.model = AutoModel.from_pretrained(clustering_model)\n",
    "        self.trained = False\n",
    "        self.n_clusters = n_clusters\n",
    "        self.output_path = output_path\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def generate_embeddings(self, mentions):\n",
    "        tokenized_inputs = self.tokenizer(mentions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized_inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        return embeddings[:, 0, :]\n",
    "\n",
    "    def train_clusters(self, mentions_train, **kwargs):\n",
    "        mentions_embeddings_train = self.generate_embeddings(mentions_train)\n",
    "        specified_params = ['random_state', 'n_init']\n",
    "        for param in specified_params:\n",
    "            if param in kwargs:\n",
    "                del kwargs[param]\n",
    "                print(\"Warning. The parameter \" + param + \" can not be changed from its given value.\")\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=0, n_init=\"auto\", **kwargs).fit(mentions_embeddings_train)\n",
    "        self.trained = True\n",
    "        self.save_trained_model()\n",
    "\n",
    "        clusters_examples = dict()\n",
    "        for i in range(max(self.kmeans.labels_)+1):\n",
    "            instances = []\n",
    "            for j in range(len(self.kmeans.labels_)):\n",
    "                if self.kmeans.labels_[j] == i and len(instances) < 5:\n",
    "                    instances.append(mentions_train[j])\n",
    "            clusters_examples[\"Class \" + str(i)] = instances\n",
    "        print(clusters_examples)\n",
    "\n",
    "    def predict_clusters(self, mentions):\n",
    "        word_with_cluster = {}\n",
    "        mentions_embeddings = self.generate_embeddings(mentions)\n",
    "        if self.model_path is not None and not self.trained:\n",
    "            self.import_pretrained_model(self.model_path)\n",
    "            self.trained = True\n",
    "        elif not self.trained:\n",
    "            self.train_clusters(mentions)\n",
    "            self.trained = True\n",
    "            self.save_trained_model()\n",
    "        predicted_clusters = self.kmeans.predict(mentions_embeddings)\n",
    "        for i in range(len(mentions)):\n",
    "            word_with_cluster[mentions[i]] = predicted_clusters[i]\n",
    "        return word_with_cluster\n",
    "    \n",
    "    def save_trained_model(self):\n",
    "        with open(self.output_path, 'wb') as file:\n",
    "            pickle.dump(self.kmeans, file)      \n",
    "\n",
    "    def import_pretrained_model(self, model_path):\n",
    "        with open(model_path, 'rb') as file:\n",
    "            self.kmeans = pickle.load(file)\n",
    "        self.trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e318d212-2dd6-4d57-bad4-844150d393be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/mnt/c/Users/Sergi/Desktop/BSC/traindata_classification.tsv'\n",
    "traindata = pd.read_csv(file_path, sep='\\t')\n",
    "traindata = traindata.sample(frac=1, random_state=42)\n",
    "traindata_head = traindata.head(100)\n",
    "traindata_head = traindata_head[traindata_head[\"label\"].isin([\"SINTOMA\",\"ENFERMEDAD\",\"SINTOMA\",\"PROCEDIMIENTO\"])]\n",
    "\n",
    "file_path = '/mnt/c/Users/Sergi/Desktop/BSC/testdata_classification.tsv'\n",
    "testdata = pd.read_csv(file_path, sep='\\t')\n",
    "testdata = testdata.sample(frac=1, random_state=42)\n",
    "testdata_head = testdata.head(30)\n",
    "testdata_head = testdata_head[testdata_head[\"label\"].isin([\"SINTOMA\",\"ENFERMEDAD\",\"SINTOMA\",\"PROCEDIMIENTO\"])]\n",
    "\n",
    "trainX = traindata_head[\"text\"].values.tolist()\n",
    "trainY = traindata_head[\"label\"].values.tolist()\n",
    "testX = testdata_head[\"text\"].values.tolist()\n",
    "testY = testdata_head[\"label\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90cd4fc0-a22b-4af1-a3c4-28366c48f84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator MultiOutputClassifier from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\"],categorizer_method=\"setfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69df16e5-01d6-48fb-9e79-7446c051ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(\"La importancia vital de la esterilización de las manos antes de una laparoscopia es esencial si hay gripe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "207dda7a-c778-4992-ab3b-06cae37de34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Keyword(text='gripe', span='[100, 104]', extraction method='textrank', score='0.2795984421205809', categorization method='setfit', class='['SPECIES']')>,\n",
       " <Keyword(text='importancia vital', span='[3, 19]', extraction method='textrank', score='0.09206970100819285', categorization method='setfit', class='['NO_CATEGORY']')>,\n",
       " <Keyword(text='laparoscopia', span='[68, 79]', extraction method='textrank', score='0.08772274252666795', categorization method='setfit', class='['PROCEDIMIENTO']')>,\n",
       " <Keyword(text='manos', span='[49, 53]', extraction method='textrank', score='0.08182759864003777', categorization method='setfit', class='[]')>,\n",
       " <Keyword(text='esterilización', span='[27, 40]', extraction method='textrank', score='0.07223214099675339', categorization method='setfit', class='['PROCEDIMIENTO']')>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27a7bb54-49ba-47f1-b14d-57845bbd7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_noparents_1epoch. Creating a new one with MEAN pooling.\n",
      "model_head.pkl not found in /mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_noparents_1epoch, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Generating Training Pairs: 100%|█████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 200.46it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 420\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 27\n",
      "  Total train batch size = 16\n",
      "Epoch:   0%|                                                                                      | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                                                                 | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   4%|██▋                                                                      | 1/27 [00:05<02:16,  5.24s/it]\u001b[A\n",
      "Iteration:   7%|█████▍                                                                   | 2/27 [00:09<01:54,  4.59s/it]\u001b[A\n",
      "Iteration:  11%|████████                                                                 | 3/27 [00:12<01:42,  4.28s/it]\u001b[A\n",
      "Iteration:  15%|██████████▊                                                              | 4/27 [00:17<01:37,  4.25s/it]\u001b[A\n",
      "Iteration:  19%|█████████████▌                                                           | 5/27 [00:21<01:32,  4.21s/it]\u001b[A\n",
      "Iteration:  22%|████████████████▏                                                        | 6/27 [00:24<01:25,  4.09s/it]\u001b[A\n",
      "Iteration:  26%|██████████████████▉                                                      | 7/27 [00:28<01:22,  4.10s/it]\u001b[A\n",
      "Iteration:  30%|█████████████████████▋                                                   | 8/27 [00:32<01:15,  3.99s/it]\u001b[A\n",
      "Iteration:  33%|████████████████████████▎                                                | 9/27 [00:36<01:11,  3.99s/it]\u001b[A\n",
      "Iteration:  37%|██████████████████████████▋                                             | 10/27 [00:40<01:07,  3.98s/it]\u001b[A\n",
      "Iteration:  41%|█████████████████████████████▎                                          | 11/27 [00:44<01:04,  4.01s/it]\u001b[A\n",
      "Iteration:  44%|████████████████████████████████                                        | 12/27 [00:48<01:00,  4.01s/it]\u001b[A\n",
      "Iteration:  48%|██████████████████████████████████▋                                     | 13/27 [00:52<00:56,  4.03s/it]\u001b[A\n",
      "Iteration:  52%|█████████████████████████████████████▎                                  | 14/27 [00:56<00:52,  4.01s/it]\u001b[A\n",
      "Iteration:  56%|████████████████████████████████████████                                | 15/27 [01:00<00:47,  3.98s/it]\u001b[A\n",
      "Iteration:  59%|██████████████████████████████████████████▋                             | 16/27 [01:04<00:43,  3.99s/it]\u001b[A\n",
      "Iteration:  63%|█████████████████████████████████████████████▎                          | 17/27 [01:07<00:38,  3.90s/it]\u001b[A\n",
      "Iteration:  67%|████████████████████████████████████████████████                        | 18/27 [01:11<00:35,  3.92s/it]\u001b[A\n",
      "Iteration:  70%|██████████████████████████████████████████████████▋                     | 19/27 [01:15<00:31,  3.93s/it]\u001b[A\n",
      "Iteration:  74%|█████████████████████████████████████████████████████▎                  | 20/27 [01:19<00:27,  3.96s/it]\u001b[A\n",
      "Iteration:  78%|████████████████████████████████████████████████████████                | 21/27 [01:23<00:23,  3.97s/it]\u001b[A\n",
      "Iteration:  81%|██████████████████████████████████████████████████████████▋             | 22/27 [01:27<00:19,  3.98s/it]\u001b[A\n",
      "Iteration:  85%|█████████████████████████████████████████████████████████████▎          | 23/27 [01:31<00:15,  3.96s/it]\u001b[A\n",
      "Iteration:  89%|████████████████████████████████████████████████████████████████        | 24/27 [01:35<00:11,  3.96s/it]\u001b[A\n",
      "Iteration:  93%|██████████████████████████████████████████████████████████████████▋     | 25/27 [01:39<00:07,  3.97s/it]\u001b[A\n",
      "Iteration:  96%|█████████████████████████████████████████████████████████████████████▎  | 26/27 [01:43<00:03,  3.98s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████| 27/27 [01:45<00:00,  3.91s/it]\u001b[A\n",
      "Epoch: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [01:45<00:00, 105.66s/it]\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.7999999999999999, 'accuracy': 0.6875, 'Classes with no given label': {'ENFERMEDAD': 2, 'SINTOMA': 1}}\n"
     ]
    }
   ],
   "source": [
    "extractor.train_classifier(trainX,trainY,testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "580a0c9b-6229-4a00-870d-bae397c86e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning. The parameter random_state can not be changed from its given value.\n",
      "{'Class 0': ['cavernoma', 'lesión', 'tumoración carnosa de consistencia densa que rodeaba la rama del nervio dentario inferior'], 'Class 1': ['retinotoxicidad por vigabatrina', 'Doppler de arterias renales y el ecocardiograma no mostraron alteraciones', 'ferritina y saturación de transferan normales', 'exploración física', 'ganglios linfáticos, siendo los 25 negativos para células tumorales'], 'Class 2': ['metástasis ganglionares retroperitoneales', 'RM craneal', 'MAVD', 'inestabilidad a la marcha', 'antisépticos locales']}\n"
     ]
    }
   ],
   "source": [
    "traindata_clusters = traindata_head[\"text\"].tolist()\n",
    "extractor.train_clustering(traindata_clusters, max_iter=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4375a-0b4a-4a23-a9e8-bc640bb92b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predclusters = self.kmeans.labels_\n",
    "\n",
    "clusters_examples = dict()\n",
    "for i in range(max(predclusters)+1):\n",
    "    for j in range(len(predclusters)):\n",
    "        if predclusters[j] == i and len(instances) < 5:\n",
    "            clusters_examples[\"Class \" + str(i)] = mentions_train[j]\n",
    "\n",
    "print(clusters_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafda742-1ff1-4b03-bf80-ea91ca0d70db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc0f6a-c05e-4aa2-ac92-e3b8272dee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e722f2-66b9-4577-adda-5910044dcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "general_path = os.getcwd().split(\"BioTermCategorizer\")[0]+\"BioTermCategorizer/\"\n",
    "sys.path.append(general_path+'biotermcategorizer/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d679dac1-4248-4c69-9fd4-af97d84e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TermExtractor import TermExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d9605d-1c9b-44db-b767-5c67541f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc47a4-c834-49d5-988f-a18c7c6bce8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef6ce4-15dd-4c16-bf22-17e3d6ba8e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "199201ba-58db-4dd2-85f2-7da7fbc1ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53702ec3-6063-4897-a4cb-e16c88da3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample texts\n",
    "text1 = \"Paciente varón de 35 años con tumoración en polo superior de teste derecho hallada de manera casual durante una autoexploración, motivo por el cual acude a consulta de urología donde se realiza exploración física, apreciando masa de 1cm aproximado de diámetro dependiente de epidídimo, y ecografía testicular, que se informa como lesión nodular sólida en cabeza de epidídimo derecho. Se realiza RMN. Confirmando masa nodular, siendo el tumor adenomatoide de epidídimo la primera posibilidad diagnóstica. Se decide, en los dos casos, resección quirúrgica de tumoración nodular en cola epidídimo derecho, sin realización de orquiectomía posterior. En ambos casos se realizó examen anátomopatológico de la pieza quirúrgica. Hallazgos histológicos macroscópicos: formación nodular de 1,5 cms (caso1) y 1,2 cms (caso 2) de consistencia firme, coloración blanquecina y bien delimitada. Microscópicamente se observa proliferación tumoral constituida por estructuras tubulares en las que la celularidad muestra núcleos redondeados y elongados sin atipia citológica y que ocasionalmente muestra citoplasmas vacuolados, todo ello compatible con tumor adenomatoide de epidídimo.\"\n",
    "text2 = \"Dos recién nacidos, varón y hembra de una misma madre y fallecidos a los 10 y 45 minutos de vida respectivamente a los que se les realizó examen necrópsico. El primero de los cadáveres, correspondiente a la hembra, fue remitido con el juicio clínico de insuficiencia respiratoria grave con sospecha de Síndrome de Potter con la constatación de oligoamnios severo; nació mediante cesárea urgente por presentación de nalgas y el test de Apgar fue 1/3/7; minutos más tarde falleció. El examen externo permitió observar una tonalidad subcianótica, facies triangular con hendiduras parpebrales mongoloides, micrognatia, raiz nasal ancha y occipucio prominente. El abdomen, globuloso, duro y ligeramente abollonado permitía la palpación de dos grandes masas ocupando ambas fosas renales y hemiabdomenes. A la apertura de cavidades destacaba la presencia de dos grandes masas renales de 10 x 8 x 5,5 cm y 12 x 8 x 6 cm con pesos de 190 y 235 gr respectivamente. Si bien se podía discernir la silueta renal, la superficie, abollonada, presentaba numerosas formaciones quísticas de contenido seroso; al corte dichos quistes mostraban un tamaño heterogéneo siendo mayores los situados a nivel cortical, dando al riñón un aspecto de esponja. Los pulmones derecho e izquierdo pesaban 17 y 15 gr (peso habitual del conjunto de 49 gr) mostrando una tonalidad rojiza uniforme; ambos se encontraban comprimidos como consecuencia de la elevación diafragmática condicionada por el gran tamaño de los riñones. El resto de los órganos no mostraba alteraciones macroscópicas significativas salvo las alteraciones posicionales derivadas de la compresión renal. En el segundo de los cadáveres, el correspondiente al varón, se observaron cambios morfológicos similares si bien el tamaño exhibido por los riñones era aún mayor, con pesos de 300 y 310 gr. El resto de las vísceras abdominales estaban comprimidas contra el diafragma. En ambos casos se realizó un estudio histológico detallado, centrado especialmente en los riñones en los que se demostraron múltiples quistes de distintos tamaños con morfología sacular a nivel cortical. Dichos quistes ocupaban la mayor parte del parénquima corticomedular si bien las zonas conservadas no mostraban alteraciones significativas salvo inmadurez focal. Dichos quistes estaban tapizados por un epitelio simple que variaba desde plano o cúbico. Los quistes medulares, de menor tamaño y más redondeados estaban tapizados por un epitelio de predominio cúbico. Después de las renales, las alteraciones más llamativas se encontraban en el hígado donde se observaron proliferación y dilatación, incluso quística, de los ductos biliares a nivel de los espacios porta. Con tales hallazgos se emitió en ambos casos el diagnóstico de enfermedad poliquística renal autosómica recesiva infantil.\"\n",
    "text3 = \"Paciente de 64 años, alérgico a penicilina y con recambio valvular aórtico por endocarditis que consultó por aparición de masa peneana de crecimiento progresivo en las últimas semanas. A la exploración física destacaba una formación excrecente y abigarrada en glande, que deformaba meato, con áreas ulceradas cubiertas de fibrina. Se palpaban adenopatías fijas y duras en ambas regiones inguinales. La radiografía de tórax y el TAC abdomino-pélvico confirmaron la presencia de adenopatías pulmonares e inguinales de gran tamaño. Con el diagnóstico de neoplasia de pene, se practicó penectomía parcial con margen de seguridad. La anatomía patológica demostró que se trataba de un sarcoma pleomórfico de pene con diferenciación osteosarcomatosa y márgenes libres de afectación. Se decidió tratamiento con dos líneas de quimioterapia consistente en adriamicina e ifosfamida pero no hubo respuesta. Ingresó de nuevo con recidiva local sangrante de gran tamaño y crecimiento rápido que provocaba obstrucción de meato con insuficiencia renal aguda. Se colocó sonda de cistostomía y se instauró tratamiento con sueroterapia, mejorando la función renal, pero con empeoramiento progresivo del estado general hasta que falleció a los 6 meses del diagnóstico.\"\n",
    "text4 = \"Mujer de 28 años sin antecedentes de interés que consultó por síndrome miccional con polaquiuria de predominio diurno y cierto grado de urgencia sin escapes urinario. El urocultivo resultó negativo por lo que se indicó tratamiento con anticolinérgicos. Ante la falta de respuesta al tratamiento, se realizó cistografía que fué normal y ecografía renovesical en la que se apreciaban imágenes quísticas parapiélicas, algunas de ellas con tabiques internos y vejiga sin lesiones. Con el fin de precisar la naturaleza de dichos quistes se solicitó TAC-abdominal, que informaba de gran quiste parapiélico en riñón derecho sin repercusión sobre la vía y una masa hipodensa suprarrenal derecha. La resonancia magnética demostró normalidad de la glándula suprarrenal y una lesión quística lobulada conteniendo numerosos septos en su interior que rodeaba al riñón derecho; en la celda renal izquierda existía una lesión de características similares pero de menor tamaño. Los hallazgos eran compatibles con linfangioma renal bilateral. Tras tres años de seguimiento la paciente continua con leve síntomatologia miccional en tratamiento, pero no ha presentado síntomas derivados de su lesión renal.\"\n",
    "text5 = \"Varón de 68 años, con antecedentes de hemorragia digestiva alta por aspirina y accidente isquémico transitorio a tratamiento crónico con trifusal (300 mg cada 12 horas), que acudió al Servicio de Urgencias del Hospital San Agustín (Avilés, Asturias), en mayo de 2006, por dolor en hemiabdomen izquierdo, intenso, continuo, de instauración súbita y acompañado de cortejo vegetativo. A la exploración presentaba una tensión arterial de 210/120 mm Hg, una frecuencia cardíaca de 80 por minuto, y dolor en fosa ilíaca izquierda, acentuado con la palpación. El hemograma (hemoglobina: 13 g/dL, plaquetas: 249.000), el estudio de coagulación, la bioquímica elemental de sangre, el sistemático de orina, el electrocardiograma y la radiografía simple de tórax eran normales. En la tomografía computarizada de abdomen se objetivó un extenso hematoma, de 12 cm de diámetro máximo, en la celda renal izquierda, sin líquido libre intraperitoneal; la suprarrenal izquierda quedaba englobada y no se podía identificar, y la derecha no presentaba alteraciones. La HTA no se llegó a controlar en Urgencias, a pesar del tratamiento con analgésicos, con antagonistas del calcio y con inhibidores de la enzima convertidora de la angiotensina II, por lo que el paciente, que mantenía cifras tensionales de 240/160 mm Hg, pasó a la Unidad de Cuidados Intensivos, para tratamiento intravenoso con nitroprusiato y labetalol. En las 24 horas siguientes se yuguló la crisis hipertensiva, y se comprobó que la hemoglobina y el hematocrito permanecían estables. Con la sospecha diagnóstica de rotura no traumática de un feocromocitoma pre-existente, se determinaron metanefrinas plasmáticas, que fueron normales, y catecolaminas y metanefrinas urinarias. En la orina de 24 horas del día siguiente al ingreso se obtuvieron los siguientes resultados: adrenalina: 65,1 mcg (valores normales -VN: 1,7-22,5), noradrenalina: 151,1 mcg (VN: 12,1-85,5), metanefrina: 853,5 mcg (VN: 74-297) y normetanefrina: 1396,6 mcg (VN: 105-354). A los 10 días, todavía ingresado el paciente, las cifras urinarias se habían normalizado por completo de modo espontáneo. Respecto al hematoma, en julio de 2006 no se había reabsorbido y persistía una imagen pseudoquística en la zona suprarrenal izquierda. En septiembre de 2006 se practicó una suprarrenalectomía unilateral, y el estudio histológico mostró una masa encapsulada de 6 x 5 cm, con necrosis hemorrágica extensa y algunas células corticales sin atipias.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f00dff96-03fd-4195-902f-cdaec9d63a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "141\n",
      "67\n",
      "52\n",
      "152\n",
      "CPU times: user 160 ms, sys: 1.38 ms, total: 161 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2656ba7-6002-4586-9dec-622fc334dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "35\n",
      "19\n",
      "20\n",
      "14\n",
      "CPU times: user 545 ms, sys: 5.49 ms, total: 551 ms\n",
      "Wall time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"yake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e47ebf00-f778-4157-8a13-0729c269cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "95\n",
      "52\n",
      "53\n",
      "85\n",
      "CPU times: user 1.04 s, sys: 88.3 ms, total: 1.12 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"textrank\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f99d9ad2-cede-4005-bb85-9829e54a1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "173\n",
      "84\n",
      "76\n",
      "164\n",
      "CPU times: user 1.76 s, sys: 97.7 ms, total: 1.86 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\",\"yake\",\"textrank\"], join=True)\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31d9bc3-e2fd-4561-b6ce-a35c13e58476",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e3947b-b249-41b3-aba4-26a27e171ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Keyword(text='realizó examen anátomopatológico', method='rake', score='9.0', spans = '[664, 695]')>\n"
     ]
    }
   ],
   "source": [
    "a = extractor.keywords[0]\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
