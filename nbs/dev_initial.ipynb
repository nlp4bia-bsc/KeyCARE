{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecb69cf-31ea-4dc6-8e0c-d7596f13ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-w230neef\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-w230neef\n",
      "  Resolved https://github.com/LIAAD/yake to commit 374fc1c1c19eb080d5b6115cbb8d4a4324392e54\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=6.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (8.1.6)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m454.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: regex in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from segtok->yake==0.4.8) (2023.6.3)\n",
      "Using legacy 'setup.py install' for yake, since package 'wheel' is not installed.\n",
      "Installing collected packages: tabulate, segtok, numpy, networkx, jellyfish, yake\n",
      "  Running setup.py install for yake ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed jellyfish-1.0.0 networkx-3.1 numpy-1.25.2 segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19908a5e-4d69-47e7-9a33-137ab2a503d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: click in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
      "Requirement already satisfied: joblib in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fb147fe-ed9d-4b84-ba89-b9ef65135ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytextrank\n",
      "  Downloading pytextrank-3.2.5-py3-none-any.whl (30 kB)\n",
      "Collecting spacy>=3.0\n",
      "  Downloading spacy-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments>=2.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (2.15.1)\n",
      "Requirement already satisfied: networkx[default]>=2.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (3.1)\n",
      "Collecting graphviz>=0.13\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting icecream>=2.1\n",
      "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting colorama>=0.3.9\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: executing>=0.3.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (2.2.1)\n",
      "Collecting pandas>=1.3\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib>=3.4\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from networkx[default]>=2.6->pytextrank) (1.25.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (914 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.3/914.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (23.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (4.65.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: six in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2023.3)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.7.1)\n",
      "Collecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, tzdata, typer, spacy-loggers, spacy-legacy, smart-open, scipy, pyparsing, pydantic-core, pillow, murmurhash, langcodes, kiwisolver, graphviz, fonttools, cycler, contourpy, colorama, catalogue, blis, annotated-types, srsly, pydantic, preshed, pathy, pandas, matplotlib, icecream, confection, thinc, spacy, pytextrank\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 colorama-0.4.6 confection-0.1.1 contourpy-1.1.0 cycler-0.11.0 cymem-2.0.7 fonttools-4.42.0 graphviz-0.20.1 icecream-2.1.3 kiwisolver-1.4.4 langcodes-3.3.0 matplotlib-3.7.2 murmurhash-1.0.9 pandas-2.0.3 pathy-0.10.2 pillow-10.0.0 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 pyparsing-3.0.9 pytextrank-3.2.5 scipy-1.11.1 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.11 typer-0.9.0 tzdata-2023.3 wasabi-1.1.2\n",
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.11)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (59.6.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pytextrank\n",
    "!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c55ce4-ab7e-48a6-abbd-d5ce385630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0653ba11-2b67-463b-a73d-8efa4c69b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "from rake_nltk import Rake\n",
    "\n",
    "import yake\n",
    "\n",
    "import spacy, pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2654b878-7d5e-48ca-89ed-357cfa13393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, language, max_tokens):\n",
    "        self.language = language\n",
    "        self.max_tokens = max_tokens\n",
    "        pass\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        raise NotImplementedError(\"extract_terms method must be implemented in subclass\")\n",
    "\n",
    "    def extract_terms_with_span(self, text):\n",
    "        terms = self.extract_terms(text)\n",
    "        terms_with_span = self.find_term_span(text, terms)\n",
    "        return terms_with_span\n",
    "\n",
    "    def extract_terms_without_overlaps(self, text):\n",
    "        terms_with_span = self.extract_terms_with_span(text)\n",
    "        terms_without_overlaps = self.rmv_overlaps(terms_with_span)\n",
    "        return terms_without_overlaps\n",
    "\n",
    "    def postprocess_terms(self, terms):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def find_term_span(text, terms):\n",
    "        spans = []\n",
    "        for t,score in terms:\n",
    "          term = re.escape(t)\n",
    "          patron = r'\\b' + term + r'\\b'\n",
    "          coincidencias = re.finditer(patron, text, re.IGNORECASE)\n",
    "          span = [(t, coincidencia.start(), coincidencia.end()-1, score) for coincidencia in coincidencias]\n",
    "          spans.extend(span)\n",
    "        return spans\n",
    "\n",
    "    @staticmethod\n",
    "    def rmv_overlaps(keywords):\n",
    "      ent = [kw[0] for kw in keywords]\n",
    "      pos = [kw[1:3] for kw in keywords]\n",
    "      score = [kw[3:] for kw in keywords]\n",
    "      updated_keywords = []\n",
    "      repeated_words = []\n",
    "      for i in range(len(ent)):\n",
    "        overlap = False\n",
    "        if pos[i] in repeated_words:\n",
    "          overlap = True\n",
    "        else:\n",
    "          repeated_words.append(pos[i])\n",
    "          for k in range(len(ent)):\n",
    "            if (((pos[i][0] >= pos[k][0]) and (pos[i][1] < pos[k][1])) or ((pos[i][0] > pos[k][0]) and (pos[i][1] <= pos[k][1]))):\n",
    "              overlap = True\n",
    "        if (not overlap):\n",
    "          kw = (ent[i],pos[i][0],pos[i][1]) + score[i]\n",
    "          updated_keywords.append(kw)\n",
    "      return updated_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e947790-f7bd-479d-be58-1923941c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "        self.extractor = Rake(stopwords=self.stopwords, language=language)\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        self.extractor.extract_keywords_from_text(text)\n",
    "        terms = self.extractor.get_ranked_phrases_with_scores()\n",
    "        terms = [(kw,score) for score,kw in terms if (len(word_tokenize(kw, language=self.language)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29313ad-4f4f-4aeb-832c-40c2aa71c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = yake.KeywordExtractor() #aqui habria que poner top=70 por ejemplo\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        keywords = self.extractor.extract_keywords(text)\n",
    "        terms = [(kw,score) for kw, score in keywords if (len(word_tokenize(kw)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f018e683-962a-4222-8a66-d6f99847d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = spacy.load(\"es_core_news_sm\")\n",
    "            self.extractor.add_pipe(\"textrank\")\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        doc = self.extractor(text)\n",
    "        terms = []\n",
    "        for phrase in doc._.phrases:\n",
    "          if (len(word_tokenize(phrase.text)) <= self.max_tokens):\n",
    "            terms.append((phrase.text, phrase.rank))\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5200cc68-2cfa-4fb1-95c1-a73291673d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    def __init__(self, extraction_methods=\"rake\", language=\"spanish\", max_tokens=3, join=False): #maybe fer que max_tokens s hagi de posar mes tard??\n",
    "        self.extraction_methods = extraction_methods\n",
    "        self.extractors = self.initialize_keyword_extractors(language, max_tokens)\n",
    "        self.keywords = None\n",
    "        self.join = join\n",
    "\n",
    "    def __call__(self, text):\n",
    "        terms = self.extract_terms(text, self.join)\n",
    "        return terms\n",
    "        \n",
    "    def initialize_keyword_extractors(self, language, max_tokens):\n",
    "        keyword_extractors = {} #esto esta hecho para mas de un extractor a la vez???\n",
    "        \n",
    "        if 'rake' in self.extraction_methods:\n",
    "            keyword_extractors[\"rake\"] = RakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'yake' in self.extraction_methods:\n",
    "            keyword_extractors[\"yake\"] = YakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'textrank' in self.extraction_methods:\n",
    "            keyword_extractors[\"textrank\"] = TextRankExtractor(language, max_tokens)\n",
    "\n",
    "        if not keyword_extractors:\n",
    "            raise ValueError(\"No extraction method called {}\".format(self.extraction_methods))\n",
    "        \n",
    "        return keyword_extractors\n",
    "\n",
    "    def extract_terms(self, text, join):\n",
    "        try:\n",
    "            all_terms = []\n",
    "            for key, extractor in self.extractors.items():\n",
    "                terms = extractor.extract_terms_without_overlaps(text)\n",
    "                terms = [term + (key,) for term in terms]\n",
    "                all_terms.extend(terms)\n",
    "            if join:\n",
    "                all_terms = list(self.extractors.values())[0].rmv_overlaps(all_terms)\n",
    "            self.keywords = [Keyword(text=i[0], method=i[4], ini=i[1], fin=i[2], score=i[3]) for i in all_terms]\n",
    "        except:\n",
    "            raise AttributeError(\"A list of extractors must be provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ae49e0-7615-4a76-b2f8-19b9f3eb9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword:\n",
    "    def __init__(self, text, method, ini, fin, score):\n",
    "        self.text = text\n",
    "        self.method = method\n",
    "        self.score = score\n",
    "        self.span = [ini, fin]\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        A method to return a string representation of the Keyword object.\n",
    "\n",
    "        Parameters:\n",
    "        - None\n",
    "\n",
    "        Returns:\n",
    "        - A string representing the Keyword object.\n",
    "        \"\"\"\n",
    "        return f\"<Keyword(text='{self.text}', method='{self.method}', score='{self.score}', spans='{self.span}')>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90cd4fc0-a22b-4af1-a3c4-28366c48f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5022439-f3c0-466f-965a-8aa47c0336a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RakeExtractor at 0x7fde096d6290>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(extractor.extractors.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69df16e5-01d6-48fb-9e79-7446c051ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(\"La importancia vital de lavarse con jabón las manos antes de operar es esencial\", join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "207dda7a-c778-4992-ab3b-06cae37de34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('operar', 'rake'), ('lavarse', 'rake'), ('jabón', 'rake'), ('esencial', 'rake'), ('La importancia vital', 'textrank'), ('las manos', 'textrank')]\n"
     ]
    }
   ],
   "source": [
    "a = [(kw.text,kw.method) for kw in extractor.keywords]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b0c30-5f8a-4647-98f6-eace29e7d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estaria be fer el post processing abans de rmv_overlaps perq aixi les que passen a ser insignificants despres del postprocessing les eliminem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8397a914-c7a7-47b4-8184-472a0cd32a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando TextRank y un máximo de 5 tokens: \n",
      " None\n",
      "\n",
      "Usando Rake y Yake juntos y un máximo de 2 tokens: \n",
      " None\n",
      "\n",
      "Usando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción completa directa\n",
    "text = \"Un varón de 32 años acude al Servicio de Urgencias por disminución reciente de visión en OD coincidiendo con la aparición de una lesión parduzca en dicho ojo. Entre los antecedentes oftalmológicos destaca un traumatismo penetrante en OD tres años antes que fue suturado en nuestro centro. A la exploración presenta una agudeza visual de 0,1 que mejora a 0,5 con estenopéico.\"\n",
    "\n",
    "extractor1 = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=5)\n",
    "print(\"Usando TextRank y un máximo de 5 tokens: \\n\",extractor1(text))\n",
    "\n",
    "extractor2 = TermExtractor(extraction_methods=[\"rake\", \"yake\"], language=\"spanish\", max_tokens=2)\n",
    "print(\"\\nUsando Rake y Yake juntos y un máximo de 2 tokens: \\n\",extractor2(text))\n",
    "\n",
    "extractor3 = TermExtractor(extraction_methods=[\"rake\", \"yake\", \"textrank\"], language=\"spanish\", max_tokens=3)\n",
    "print(\"\\nUsando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \\n\",extractor3(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9d95c8-a49f-4823-bb89-dcdb8ef6aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dicho ojo', 148, 156), ('disminución reciente', 55, 74), ('OD', 89, 90), ('OD', 234, 235), ('estenopéico', 362, 372), ('visión', 79, 84), ('una lesión parduzca', 125, 143), ('Servicio de Urgencias', 29, 49), ('un traumatismo penetrante', 205, 229), ('nuestro centro', 273, 286), ('una agudeza visual', 315, 332), ('la aparición', 109, 120), ('32 años', 12, 18), ('tres años', 237, 245), ('los antecedentes oftalmológicos', 165, 195), ('Un varón', 0, 7), ('A la exploración', 289, 304), ('que', 253, 255), ('que', 341, 343)]\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción por pasos: vemos que da lo mismo que el de antes\n",
    "extractor = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=3)\n",
    "terms = extractor.extractors[\"textrank\"].extract_terms(text)\n",
    "terms_with_span = extractor.extractors[\"textrank\"].find_term_span(text,terms)\n",
    "terms_without_overlaps = extractor.extractors[\"textrank\"].rmv_overlaps(terms_with_span)\n",
    "print(terms_without_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b02f74d-aa21-468f-bf80-a2ba888154da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348c6565-7bfd-4065-92a4-5d54b191f04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['importancia vital', 'operar', 'manos', 'lavarse', 'jabón', 'indispensable']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].extract_terms(\"La importancia vital de lavarse con jabón las manos antes de operar es indispensable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c4e94-b6d8-4b86-9c60-2fb1f3382f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_terminos = TermExtractor(sadasdasdsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4f1f2-3605-4daa-8e94-8c8859cd6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms = extractor_terminos(\"textoasdasdasdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4089de-7798-443f-a475-fd0f37b6a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms.zº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33845a0-6647-4236-b667-89b62d67c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e722f2-66b9-4577-adda-5910044dcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "general_path = os.getcwd().split(\"BioTermCategorizer\")[0]+\"BioTermCategorizer/\"\n",
    "sys.path.append(general_path+'biotermcategorizer/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d679dac1-4248-4c69-9fd4-af97d84e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TermExtractor import TermExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d9605d-1c9b-44db-b767-5c67541f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "199201ba-58db-4dd2-85f2-7da7fbc1ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53702ec3-6063-4897-a4cb-e16c88da3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample texts\n",
    "text1 = \"Paciente varón de 35 años con tumoración en polo superior de teste derecho hallada de manera casual durante una autoexploración, motivo por el cual acude a consulta de urología donde se realiza exploración física, apreciando masa de 1cm aproximado de diámetro dependiente de epidídimo, y ecografía testicular, que se informa como lesión nodular sólida en cabeza de epidídimo derecho. Se realiza RMN. Confirmando masa nodular, siendo el tumor adenomatoide de epidídimo la primera posibilidad diagnóstica. Se decide, en los dos casos, resección quirúrgica de tumoración nodular en cola epidídimo derecho, sin realización de orquiectomía posterior. En ambos casos se realizó examen anátomopatológico de la pieza quirúrgica. Hallazgos histológicos macroscópicos: formación nodular de 1,5 cms (caso1) y 1,2 cms (caso 2) de consistencia firme, coloración blanquecina y bien delimitada. Microscópicamente se observa proliferación tumoral constituida por estructuras tubulares en las que la celularidad muestra núcleos redondeados y elongados sin atipia citológica y que ocasionalmente muestra citoplasmas vacuolados, todo ello compatible con tumor adenomatoide de epidídimo.\"\n",
    "text2 = \"Dos recién nacidos, varón y hembra de una misma madre y fallecidos a los 10 y 45 minutos de vida respectivamente a los que se les realizó examen necrópsico. El primero de los cadáveres, correspondiente a la hembra, fue remitido con el juicio clínico de insuficiencia respiratoria grave con sospecha de Síndrome de Potter con la constatación de oligoamnios severo; nació mediante cesárea urgente por presentación de nalgas y el test de Apgar fue 1/3/7; minutos más tarde falleció. El examen externo permitió observar una tonalidad subcianótica, facies triangular con hendiduras parpebrales mongoloides, micrognatia, raiz nasal ancha y occipucio prominente. El abdomen, globuloso, duro y ligeramente abollonado permitía la palpación de dos grandes masas ocupando ambas fosas renales y hemiabdomenes. A la apertura de cavidades destacaba la presencia de dos grandes masas renales de 10 x 8 x 5,5 cm y 12 x 8 x 6 cm con pesos de 190 y 235 gr respectivamente. Si bien se podía discernir la silueta renal, la superficie, abollonada, presentaba numerosas formaciones quísticas de contenido seroso; al corte dichos quistes mostraban un tamaño heterogéneo siendo mayores los situados a nivel cortical, dando al riñón un aspecto de esponja. Los pulmones derecho e izquierdo pesaban 17 y 15 gr (peso habitual del conjunto de 49 gr) mostrando una tonalidad rojiza uniforme; ambos se encontraban comprimidos como consecuencia de la elevación diafragmática condicionada por el gran tamaño de los riñones. El resto de los órganos no mostraba alteraciones macroscópicas significativas salvo las alteraciones posicionales derivadas de la compresión renal. En el segundo de los cadáveres, el correspondiente al varón, se observaron cambios morfológicos similares si bien el tamaño exhibido por los riñones era aún mayor, con pesos de 300 y 310 gr. El resto de las vísceras abdominales estaban comprimidas contra el diafragma. En ambos casos se realizó un estudio histológico detallado, centrado especialmente en los riñones en los que se demostraron múltiples quistes de distintos tamaños con morfología sacular a nivel cortical. Dichos quistes ocupaban la mayor parte del parénquima corticomedular si bien las zonas conservadas no mostraban alteraciones significativas salvo inmadurez focal. Dichos quistes estaban tapizados por un epitelio simple que variaba desde plano o cúbico. Los quistes medulares, de menor tamaño y más redondeados estaban tapizados por un epitelio de predominio cúbico. Después de las renales, las alteraciones más llamativas se encontraban en el hígado donde se observaron proliferación y dilatación, incluso quística, de los ductos biliares a nivel de los espacios porta. Con tales hallazgos se emitió en ambos casos el diagnóstico de enfermedad poliquística renal autosómica recesiva infantil.\"\n",
    "text3 = \"Paciente de 64 años, alérgico a penicilina y con recambio valvular aórtico por endocarditis que consultó por aparición de masa peneana de crecimiento progresivo en las últimas semanas. A la exploración física destacaba una formación excrecente y abigarrada en glande, que deformaba meato, con áreas ulceradas cubiertas de fibrina. Se palpaban adenopatías fijas y duras en ambas regiones inguinales. La radiografía de tórax y el TAC abdomino-pélvico confirmaron la presencia de adenopatías pulmonares e inguinales de gran tamaño. Con el diagnóstico de neoplasia de pene, se practicó penectomía parcial con margen de seguridad. La anatomía patológica demostró que se trataba de un sarcoma pleomórfico de pene con diferenciación osteosarcomatosa y márgenes libres de afectación. Se decidió tratamiento con dos líneas de quimioterapia consistente en adriamicina e ifosfamida pero no hubo respuesta. Ingresó de nuevo con recidiva local sangrante de gran tamaño y crecimiento rápido que provocaba obstrucción de meato con insuficiencia renal aguda. Se colocó sonda de cistostomía y se instauró tratamiento con sueroterapia, mejorando la función renal, pero con empeoramiento progresivo del estado general hasta que falleció a los 6 meses del diagnóstico.\"\n",
    "text4 = \"Mujer de 28 años sin antecedentes de interés que consultó por síndrome miccional con polaquiuria de predominio diurno y cierto grado de urgencia sin escapes urinario. El urocultivo resultó negativo por lo que se indicó tratamiento con anticolinérgicos. Ante la falta de respuesta al tratamiento, se realizó cistografía que fué normal y ecografía renovesical en la que se apreciaban imágenes quísticas parapiélicas, algunas de ellas con tabiques internos y vejiga sin lesiones. Con el fin de precisar la naturaleza de dichos quistes se solicitó TAC-abdominal, que informaba de gran quiste parapiélico en riñón derecho sin repercusión sobre la vía y una masa hipodensa suprarrenal derecha. La resonancia magnética demostró normalidad de la glándula suprarrenal y una lesión quística lobulada conteniendo numerosos septos en su interior que rodeaba al riñón derecho; en la celda renal izquierda existía una lesión de características similares pero de menor tamaño. Los hallazgos eran compatibles con linfangioma renal bilateral. Tras tres años de seguimiento la paciente continua con leve síntomatologia miccional en tratamiento, pero no ha presentado síntomas derivados de su lesión renal.\"\n",
    "text5 = \"Varón de 68 años, con antecedentes de hemorragia digestiva alta por aspirina y accidente isquémico transitorio a tratamiento crónico con trifusal (300 mg cada 12 horas), que acudió al Servicio de Urgencias del Hospital San Agustín (Avilés, Asturias), en mayo de 2006, por dolor en hemiabdomen izquierdo, intenso, continuo, de instauración súbita y acompañado de cortejo vegetativo. A la exploración presentaba una tensión arterial de 210/120 mm Hg, una frecuencia cardíaca de 80 por minuto, y dolor en fosa ilíaca izquierda, acentuado con la palpación. El hemograma (hemoglobina: 13 g/dL, plaquetas: 249.000), el estudio de coagulación, la bioquímica elemental de sangre, el sistemático de orina, el electrocardiograma y la radiografía simple de tórax eran normales. En la tomografía computarizada de abdomen se objetivó un extenso hematoma, de 12 cm de diámetro máximo, en la celda renal izquierda, sin líquido libre intraperitoneal; la suprarrenal izquierda quedaba englobada y no se podía identificar, y la derecha no presentaba alteraciones. La HTA no se llegó a controlar en Urgencias, a pesar del tratamiento con analgésicos, con antagonistas del calcio y con inhibidores de la enzima convertidora de la angiotensina II, por lo que el paciente, que mantenía cifras tensionales de 240/160 mm Hg, pasó a la Unidad de Cuidados Intensivos, para tratamiento intravenoso con nitroprusiato y labetalol. En las 24 horas siguientes se yuguló la crisis hipertensiva, y se comprobó que la hemoglobina y el hematocrito permanecían estables. Con la sospecha diagnóstica de rotura no traumática de un feocromocitoma pre-existente, se determinaron metanefrinas plasmáticas, que fueron normales, y catecolaminas y metanefrinas urinarias. En la orina de 24 horas del día siguiente al ingreso se obtuvieron los siguientes resultados: adrenalina: 65,1 mcg (valores normales -VN: 1,7-22,5), noradrenalina: 151,1 mcg (VN: 12,1-85,5), metanefrina: 853,5 mcg (VN: 74-297) y normetanefrina: 1396,6 mcg (VN: 105-354). A los 10 días, todavía ingresado el paciente, las cifras urinarias se habían normalizado por completo de modo espontáneo. Respecto al hematoma, en julio de 2006 no se había reabsorbido y persistía una imagen pseudoquística en la zona suprarrenal izquierda. En septiembre de 2006 se practicó una suprarrenalectomía unilateral, y el estudio histológico mostró una masa encapsulada de 6 x 5 cm, con necrosis hemorrágica extensa y algunas células corticales sin atipias.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f00dff96-03fd-4195-902f-cdaec9d63a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "141\n",
      "67\n",
      "52\n",
      "152\n",
      "CPU times: user 160 ms, sys: 1.38 ms, total: 161 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2656ba7-6002-4586-9dec-622fc334dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "35\n",
      "19\n",
      "20\n",
      "14\n",
      "CPU times: user 545 ms, sys: 5.49 ms, total: 551 ms\n",
      "Wall time: 589 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"yake\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e47ebf00-f778-4157-8a13-0729c269cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "95\n",
      "52\n",
      "53\n",
      "85\n",
      "CPU times: user 1.04 s, sys: 88.3 ms, total: 1.12 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"textrank\"])\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f99d9ad2-cede-4005-bb85-9829e54a1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "173\n",
      "84\n",
      "76\n",
      "164\n",
      "CPU times: user 1.76 s, sys: 97.7 ms, total: 1.86 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extractor = TermExtractor(extraction_methods=[\"rake\",\"yake\",\"textrank\"], join=True)\n",
    "extractor(text1)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text2)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text3)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text4)\n",
    "print(len(extractor.keywords))\n",
    "extractor(text5)\n",
    "print(len(extractor.keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31d9bc3-e2fd-4561-b6ce-a35c13e58476",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e3947b-b249-41b3-aba4-26a27e171ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Keyword(text='realizó examen anátomopatológico', method='rake', score='9.0', spans = '[664, 695]')>\n"
     ]
    }
   ],
   "source": [
    "a = extractor.keywords[0]\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
