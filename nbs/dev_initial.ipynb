{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecb69cf-31ea-4dc6-8e0c-d7596f13ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/LIAAD/yake\n",
      "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-w230neef\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-w230neef\n",
      "  Resolved https://github.com/LIAAD/yake to commit 374fc1c1c19eb080d5b6115cbb8d4a4324392e54\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click>=6.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from yake==0.4.8) (8.1.6)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m454.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: regex in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from segtok->yake==0.4.8) (2023.6.3)\n",
      "Using legacy 'setup.py install' for yake, since package 'wheel' is not installed.\n",
      "Installing collected packages: tabulate, segtok, numpy, networkx, jellyfish, yake\n",
      "  Running setup.py install for yake ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed jellyfish-1.0.0 networkx-3.1 numpy-1.25.2 segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19908a5e-4d69-47e7-9a33-137ab2a503d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: click in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2023.6.3)\n",
      "Requirement already satisfied: joblib in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fb147fe-ed9d-4b84-ba89-b9ef65135ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytextrank\n",
      "  Downloading pytextrank-3.2.5-py3-none-any.whl (30 kB)\n",
      "Collecting spacy>=3.0\n",
      "  Downloading spacy-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments>=2.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (2.15.1)\n",
      "Requirement already satisfied: networkx[default]>=2.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pytextrank) (3.1)\n",
      "Collecting graphviz>=0.13\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting icecream>=2.1\n",
      "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting colorama>=0.3.9\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: executing>=0.3.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from icecream>=2.1->pytextrank) (2.2.1)\n",
      "Collecting pandas>=1.3\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib>=3.4\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from networkx[default]>=2.6->pytextrank) (1.25.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (914 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.3/914.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (23.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (4.65.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy>=3.0->pytextrank) (3.1.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Requirement already satisfied: six in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from matplotlib>=3.4->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pandas>=1.3->networkx[default]>=2.6->pytextrank) (2023.3)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.7.1)\n",
      "Collecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.0.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from jinja2->spacy>=3.0->pytextrank) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, tzdata, typer, spacy-loggers, spacy-legacy, smart-open, scipy, pyparsing, pydantic-core, pillow, murmurhash, langcodes, kiwisolver, graphviz, fonttools, cycler, contourpy, colorama, catalogue, blis, annotated-types, srsly, pydantic, preshed, pathy, pandas, matplotlib, icecream, confection, thinc, spacy, pytextrank\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 colorama-0.4.6 confection-0.1.1 contourpy-1.1.0 cycler-0.11.0 cymem-2.0.7 fonttools-4.42.0 graphviz-0.20.1 icecream-2.1.3 kiwisolver-1.4.4 langcodes-3.3.0 matplotlib-3.7.2 murmurhash-1.0.9 pandas-2.0.3 pathy-0.10.2 pillow-10.0.0 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 pyparsing-3.0.9 pytextrank-3.2.5 scipy-1.11.1 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.11 typer-0.9.0 tzdata-2023.3 wasabi-1.1.2\n",
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.11)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (59.6.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install pytextrank\n",
    "!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0653ba11-2b67-463b-a73d-8efa4c69b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sergi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from rake_nltk import Rake\n",
    "\n",
    "import yake\n",
    "\n",
    "import spacy, pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2654b878-7d5e-48ca-89ed-357cfa13393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, language, max_tokens):\n",
    "        self.language = language\n",
    "        self.max_tokens = max_tokens\n",
    "        pass\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        raise NotImplementedError(\"extract_terms method must be implemented in subclass\")\n",
    "\n",
    "    def extract_terms_with_span(self, text):\n",
    "        terms = self.extract_terms(text)\n",
    "        terms_with_span = self.find_term_span(text, terms)\n",
    "        return terms_with_span\n",
    "\n",
    "    def extract_terms_without_overlaps(self, text):\n",
    "        terms_with_span = self.extract_terms_with_span(text)\n",
    "        terms_without_overlaps = self.rmv_overlaps(terms_with_span)\n",
    "        return terms_without_overlaps\n",
    "\n",
    "    def postprocess_terms(self, terms):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def find_term_span(text, terms):\n",
    "        spans = []\n",
    "        for t in terms:\n",
    "          term = re.escape(t)\n",
    "          patron = r'\\b' + term + r'\\b'\n",
    "          coincidencias = re.finditer(patron, text, re.IGNORECASE)\n",
    "          span = [[coincidencia.start(), coincidencia.end()-1] for coincidencia in coincidencias]\n",
    "          spans.append((t,span))\n",
    "        return spans\n",
    "\n",
    "    @staticmethod\n",
    "    def rmv_overlaps(keywords):\n",
    "      ent = [kw[0] for kw in keywords]\n",
    "      pos = [kw[1] for kw in keywords]\n",
    "      updated_keywords = []\n",
    "      n = 0\n",
    "      for i in range(len(ent)):\n",
    "        for j in range(len(pos[i])):\n",
    "          overlap = False\n",
    "          for k in range(len(ent)):\n",
    "            for l in range(len(pos[k])):\n",
    "              if ((pos[i][j][0] >= pos[k][l][0]) and (pos[i][j][1] <= pos[k][l][1]) and i!=k):\n",
    "                overlap = True\n",
    "          if (not overlap):\n",
    "            updated_keywords.append((ent[i],pos[i][j][0],pos[i][j][1]))\n",
    "      return updated_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e947790-f7bd-479d-be58-1923941c312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "        self.extractor = Rake(stopwords=self.stopwords, language=language)\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        self.extractor.extract_keywords_from_text(text)\n",
    "        terms = self.extractor.get_ranked_phrases()\n",
    "        terms = [kw for kw in terms if (len(word_tokenize(kw)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29313ad-4f4f-4aeb-832c-40c2aa71c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YakeExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = yake.KeywordExtractor() #aqui habria que poner top=70 por ejemplo\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        keywords = self.extractor.extract_keywords(text)\n",
    "        terms = [kw for kw, score in keywords if (len(word_tokenize(kw)) <= self.max_tokens)]\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f018e683-962a-4222-8a66-d6f99847d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankExtractor(Extractor):\n",
    "    def __init__(self, language, max_tokens):\n",
    "        super().__init__(language, max_tokens)\n",
    "        if language=='spanish':\n",
    "            self.extractor = spacy.load(\"es_core_news_sm\")\n",
    "            self.extractor.add_pipe(\"textrank\")\n",
    "        else:\n",
    "            raise ValueError(\"Expected spanish language. Other languages not recognised\")\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        doc = self.extractor(text)\n",
    "        terms = []\n",
    "        for phrase in doc._.phrases:\n",
    "          if (len(word_tokenize(phrase.text)) <= self.max_tokens):\n",
    "            terms.append(phrase.text)\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5200cc68-2cfa-4fb1-95c1-a73291673d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    def __init__(self, extraction_methods=\"rake\", language=\"spanish\", max_tokens=3): #maybe fer que max_tokens s hagi de posar mes tard??\n",
    "        self.extraction_methods = extraction_methods\n",
    "        self.extractors = self.initialize_keyword_extractors(language, max_tokens)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        terms = self.extract_terms(text)\n",
    "        return terms\n",
    "        \n",
    "    def initialize_keyword_extractors(self, language, max_tokens):\n",
    "        keyword_extractors = {} #esto esta hecho para mas de un extractor a la vez???\n",
    "        \n",
    "        if 'rake' in self.extraction_methods:\n",
    "            keyword_extractors[\"rake\"] = RakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'yake' in self.extraction_methods:\n",
    "            keyword_extractors[\"yake\"] = YakeExtractor(language, max_tokens)\n",
    "        \n",
    "        if 'textrank' in self.extraction_methods:\n",
    "            keyword_extractors[\"textrank\"] = TextRankExtractor(language, max_tokens)\n",
    "\n",
    "        if not keyword_extractors:\n",
    "            raise ValueError(\"No extraction method called {}\".format(self.extraction_methods))\n",
    "        \n",
    "        return keyword_extractors\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        if (len(self.extractors) == 1):\n",
    "            terms = self.extractors[self.extraction_methods].extract_terms_without_overlaps(text)\n",
    "        else:\n",
    "            all_terms = []\n",
    "            for extractor in self.extractors.values():\n",
    "                all_terms.extend(extractor.extract_terms_with_span(text))\n",
    "            terms = self.extractors[self.extraction_methods[0]].rmv_overlaps(all_terms)\n",
    "        return terms\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cd4fc0-a22b-4af1-a3c4-28366c48f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69df16e5-01d6-48fb-9e79-7446c051ee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jabón', 36, 40), ('La importancia vital', 0, 19), ('las manos', 42, 50)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(\"La importancia vital de lavarse con jabón las manos antes de operar es esencial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b0c30-5f8a-4647-98f6-eace29e7d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estaria be fer el post processing abans de rmv_overlaps perq aixi les que passen a ser insignificants despres del postprocessing les eliminem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8397a914-c7a7-47b4-8184-472a0cd32a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando TextRank y un máximo de 5 tokens: \n",
      " [('dicho ojo', 148, 156), ('disminución reciente', 55, 74), ('OD', 89, 90), ('OD', 234, 235), ('estenopéico', 362, 372), ('visión', 79, 84), ('una lesión parduzca', 125, 143), ('Servicio de Urgencias', 29, 49), ('un traumatismo penetrante', 205, 229), ('nuestro centro', 273, 286), ('una agudeza visual', 315, 332), ('la aparición', 109, 120), ('32 años', 12, 18), ('tres años', 237, 245), ('Entre los antecedentes oftalmológicos', 159, 195), ('Un varón', 0, 7), ('A la exploración', 289, 304), ('que', 253, 255), ('que', 341, 343)]\n",
      "\n",
      "Usando Rake y Yake juntos y un máximo de 2 tokens: \n",
      " [('od coincidiendo', 89, 103), ('traumatismo penetrante', 208, 229), ('exploración presenta', 294, 313), ('agudeza visual', 319, 332), ('visión', 79, 84), ('varón', 3, 7), ('suturado', 261, 268), ('mejora', 345, 350), ('estenopéico', 362, 372), ('centro', 281, 286), ('aparición', 112, 120), ('5', 356, 356), ('1', 339, 339), ('Urgencias por', 41, 53), ('por disminución', 51, 65), ('años acude', 15, 24), ('coincidiendo con', 92, 107), ('una lesión', 125, 134)]\n",
      "\n",
      "Usando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \n",
      " [('antecedentes oftalmológicos destaca', 169, 203), ('32 años acude', 12, 24), ('od tres años', 234, 245), ('od coincidiendo', 89, 103), ('exploración presenta', 294, 313), ('suturado', 261, 268), ('mejora', 345, 350), ('5', 356, 356), ('1', 339, 339), ('Urgencias por disminución', 41, 65), ('por disminución reciente', 51, 74), ('acude al Servicio', 20, 36), ('reciente de visión', 67, 84), ('parduzca en dicho', 136, 152), ('tres años antes', 237, 251), ('coincidiendo con', 92, 107), ('Entre los antecedentes', 159, 180), ('un traumatismo penetrante', 205, 229), ('nuestro centro', 273, 286), ('una agudeza visual', 315, 332), ('la aparición', 109, 120), ('Un varón', 0, 7), ('A la exploración', 289, 304), ('que', 253, 255), ('que', 341, 343)]\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción completa directa\n",
    "text = \"Un varón de 32 años acude al Servicio de Urgencias por disminución reciente de visión en OD coincidiendo con la aparición de una lesión parduzca en dicho ojo. Entre los antecedentes oftalmológicos destaca un traumatismo penetrante en OD tres años antes que fue suturado en nuestro centro. A la exploración presenta una agudeza visual de 0,1 que mejora a 0,5 con estenopéico.\"\n",
    "\n",
    "extractor1 = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=5)\n",
    "print(\"Usando TextRank y un máximo de 5 tokens: \\n\",extractor1(text))\n",
    "\n",
    "extractor2 = TermExtractor(extraction_methods=[\"rake\", \"yake\"], language=\"spanish\", max_tokens=2)\n",
    "print(\"\\nUsando Rake y Yake juntos y un máximo de 2 tokens: \\n\",extractor2(text))\n",
    "\n",
    "extractor3 = TermExtractor(extraction_methods=[\"rake\", \"yake\", \"textrank\"], language=\"spanish\", max_tokens=3)\n",
    "print(\"\\nUsando Rake, Yake y TextRank juntos y un máximo de 3 tokens: \\n\",extractor3(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9d95c8-a49f-4823-bb89-dcdb8ef6aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dicho ojo', 148, 156), ('disminución reciente', 55, 74), ('OD', 89, 90), ('OD', 234, 235), ('estenopéico', 362, 372), ('visión', 79, 84), ('una lesión parduzca', 125, 143), ('Servicio de Urgencias', 29, 49), ('un traumatismo penetrante', 205, 229), ('nuestro centro', 273, 286), ('una agudeza visual', 315, 332), ('la aparición', 109, 120), ('32 años', 12, 18), ('tres años', 237, 245), ('los antecedentes oftalmológicos', 165, 195), ('Un varón', 0, 7), ('A la exploración', 289, 304), ('que', 253, 255), ('que', 341, 343)]\n"
     ]
    }
   ],
   "source": [
    "#Test de extracción por pasos: vemos que da lo mismo que el de antes\n",
    "extractor = TermExtractor(extraction_methods=\"textrank\", language=\"spanish\", max_tokens=3)\n",
    "terms = extractor.extractors[\"textrank\"].extract_terms(text)\n",
    "terms_with_span = extractor.extractors[\"textrank\"].find_term_span(text,terms)\n",
    "terms_without_overlaps = extractor.extractors[\"textrank\"].rmv_overlaps(terms_with_span)\n",
    "print(terms_without_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b02f74d-aa21-468f-bf80-a2ba888154da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo',\n",
       " 'como',\n",
       " 'más',\n",
       " 'pero',\n",
       " 'sus',\n",
       " 'le',\n",
       " 'ya',\n",
       " 'o',\n",
       " 'este',\n",
       " 'sí',\n",
       " 'porque',\n",
       " 'esta',\n",
       " 'entre',\n",
       " 'cuando',\n",
       " 'muy',\n",
       " 'sin',\n",
       " 'sobre',\n",
       " 'también',\n",
       " 'me',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'donde',\n",
       " 'quien',\n",
       " 'desde',\n",
       " 'todo',\n",
       " 'nos',\n",
       " 'durante',\n",
       " 'todos',\n",
       " 'uno',\n",
       " 'les',\n",
       " 'ni',\n",
       " 'contra',\n",
       " 'otros',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'ante',\n",
       " 'ellos',\n",
       " 'e',\n",
       " 'esto',\n",
       " 'mí',\n",
       " 'antes',\n",
       " 'algunos',\n",
       " 'qué',\n",
       " 'unos',\n",
       " 'yo',\n",
       " 'otro',\n",
       " 'otras',\n",
       " 'otra',\n",
       " 'él',\n",
       " 'tanto',\n",
       " 'esa',\n",
       " 'estos',\n",
       " 'mucho',\n",
       " 'quienes',\n",
       " 'nada',\n",
       " 'muchos',\n",
       " 'cual',\n",
       " 'poco',\n",
       " 'ella',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'algunas',\n",
       " 'algo',\n",
       " 'nosotros',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'tú',\n",
       " 'te',\n",
       " 'ti',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'ellas',\n",
       " 'nosotras',\n",
       " 'vosotros',\n",
       " 'vosotras',\n",
       " 'os',\n",
       " 'mío',\n",
       " 'mía',\n",
       " 'míos',\n",
       " 'mías',\n",
       " 'tuyo',\n",
       " 'tuya',\n",
       " 'tuyos',\n",
       " 'tuyas',\n",
       " 'suyo',\n",
       " 'suya',\n",
       " 'suyos',\n",
       " 'suyas',\n",
       " 'nuestro',\n",
       " 'nuestra',\n",
       " 'nuestros',\n",
       " 'nuestras',\n",
       " 'vuestro',\n",
       " 'vuestra',\n",
       " 'vuestros',\n",
       " 'vuestras',\n",
       " 'esos',\n",
       " 'esas',\n",
       " 'estoy',\n",
       " 'estás',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'esté',\n",
       " 'estés',\n",
       " 'estemos',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estaré',\n",
       " 'estarás',\n",
       " 'estará',\n",
       " 'estaremos',\n",
       " 'estaréis',\n",
       " 'estarán',\n",
       " 'estaría',\n",
       " 'estarías',\n",
       " 'estaríamos',\n",
       " 'estaríais',\n",
       " 'estarían',\n",
       " 'estaba',\n",
       " 'estabas',\n",
       " 'estábamos',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estuve',\n",
       " 'estuviste',\n",
       " 'estuvo',\n",
       " 'estuvimos',\n",
       " 'estuvisteis',\n",
       " 'estuvieron',\n",
       " 'estuviera',\n",
       " 'estuvieras',\n",
       " 'estuviéramos',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuviese',\n",
       " 'estuvieses',\n",
       " 'estuviésemos',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estando',\n",
       " 'estado',\n",
       " 'estada',\n",
       " 'estados',\n",
       " 'estadas',\n",
       " 'estad',\n",
       " 'he',\n",
       " 'has',\n",
       " 'ha',\n",
       " 'hemos',\n",
       " 'habéis',\n",
       " 'han',\n",
       " 'haya',\n",
       " 'hayas',\n",
       " 'hayamos',\n",
       " 'hayáis',\n",
       " 'hayan',\n",
       " 'habré',\n",
       " 'habrás',\n",
       " 'habrá',\n",
       " 'habremos',\n",
       " 'habréis',\n",
       " 'habrán',\n",
       " 'habría',\n",
       " 'habrías',\n",
       " 'habríamos',\n",
       " 'habríais',\n",
       " 'habrían',\n",
       " 'había',\n",
       " 'habías',\n",
       " 'habíamos',\n",
       " 'habíais',\n",
       " 'habían',\n",
       " 'hube',\n",
       " 'hubiste',\n",
       " 'hubo',\n",
       " 'hubimos',\n",
       " 'hubisteis',\n",
       " 'hubieron',\n",
       " 'hubiera',\n",
       " 'hubieras',\n",
       " 'hubiéramos',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubiese',\n",
       " 'hubieses',\n",
       " 'hubiésemos',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'habiendo',\n",
       " 'habido',\n",
       " 'habida',\n",
       " 'habidos',\n",
       " 'habidas',\n",
       " 'soy',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'somos',\n",
       " 'sois',\n",
       " 'son',\n",
       " 'sea',\n",
       " 'seas',\n",
       " 'seamos',\n",
       " 'seáis',\n",
       " 'sean',\n",
       " 'seré',\n",
       " 'serás',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'seréis',\n",
       " 'serán',\n",
       " 'sería',\n",
       " 'serías',\n",
       " 'seríamos',\n",
       " 'seríais',\n",
       " 'serían',\n",
       " 'era',\n",
       " 'eras',\n",
       " 'éramos',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'fui',\n",
       " 'fuiste',\n",
       " 'fue',\n",
       " 'fuimos',\n",
       " 'fuisteis',\n",
       " 'fueron',\n",
       " 'fuera',\n",
       " 'fueras',\n",
       " 'fuéramos',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fuese',\n",
       " 'fueses',\n",
       " 'fuésemos',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'sintiendo',\n",
       " 'sentido',\n",
       " 'sentida',\n",
       " 'sentidos',\n",
       " 'sentidas',\n",
       " 'siente',\n",
       " 'sentid',\n",
       " 'tengo',\n",
       " 'tienes',\n",
       " 'tiene',\n",
       " 'tenemos',\n",
       " 'tenéis',\n",
       " 'tienen',\n",
       " 'tenga',\n",
       " 'tengas',\n",
       " 'tengamos',\n",
       " 'tengáis',\n",
       " 'tengan',\n",
       " 'tendré',\n",
       " 'tendrás',\n",
       " 'tendrá',\n",
       " 'tendremos',\n",
       " 'tendréis',\n",
       " 'tendrán',\n",
       " 'tendría',\n",
       " 'tendrías',\n",
       " 'tendríamos',\n",
       " 'tendríais',\n",
       " 'tendrían',\n",
       " 'tenía',\n",
       " 'tenías',\n",
       " 'teníamos',\n",
       " 'teníais',\n",
       " 'tenían',\n",
       " 'tuve',\n",
       " 'tuviste',\n",
       " 'tuvo',\n",
       " 'tuvimos',\n",
       " 'tuvisteis',\n",
       " 'tuvieron',\n",
       " 'tuviera',\n",
       " 'tuvieras',\n",
       " 'tuviéramos',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuviese',\n",
       " 'tuvieses',\n",
       " 'tuviésemos',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'teniendo',\n",
       " 'tenido',\n",
       " 'tenida',\n",
       " 'tenidos',\n",
       " 'tenidas',\n",
       " 'tened']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348c6565-7bfd-4065-92a4-5d54b191f04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['importancia vital', 'operar', 'manos', 'lavarse', 'jabón', 'indispensable']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extractors[\"rake\"].extract_terms(\"La importancia vital de lavarse con jabón las manos antes de operar es indispensable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c4e94-b6d8-4b86-9c60-2fb1f3382f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_terminos = TermExtractor(sadasdasdsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4f1f2-3605-4daa-8e94-8c8859cd6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms = extractor_terminos(\"textoasdasdasdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4089de-7798-443f-a475-fd0f37b6a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_terms.zº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33845a0-6647-4236-b667-89b62d67c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44e722f2-66b9-4577-adda-5910044dcb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "general_path = os.getcwd().split(\"BioTermCategorizer\")[0]+\"BioTermCategorizer/\"\n",
    "sys.path.append(general_path+'biotermcategorizer/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d679dac1-4248-4c69-9fd4-af97d84e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TermExtractor import TermExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d9605d-1c9b-44db-b767-5c67541f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TermExtractor(extraction_methods=[\"textrank\",\"rake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "199201ba-58db-4dd2-85f2-7da7fbc1ede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa02ced0-1024-418e-9191-3933998c585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rake': 'OBJETO YAKE EXTRACTOR', 'textrank': 'OBJETO TEXTRANK EXTRACTOR'}\n",
      "texto de prueba\n"
     ]
    }
   ],
   "source": [
    "extractor(\"texto de prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53702ec3-6063-4897-a4cb-e16c88da3067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dff96-03fd-4195-902f-cdaec9d63a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2656ba7-6002-4586-9dec-622fc334dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd43326b-ab12-4653-910b-e133865de1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_extractor = RakeExtractor(stopwords=[\"a\",\"b\",\"c\"], language = \"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd109708-b6c2-43f8-9d5e-1c81ac43bf92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "extract_terms method must be implemented in subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrake_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhola que tal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m, in \u001b[0;36mExtractor.extract_terms\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract_terms method must be implemented in subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: extract_terms method must be implemented in subclass"
     ]
    }
   ],
   "source": [
    "rake_extractor.extract_terms(\"hola que tal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ebf00-f778-4157-8a13-0729c269cb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
