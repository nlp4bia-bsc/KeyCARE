{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe6ab7f-403a-432e-8f0f-3ce932e985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/mnt/c/Users/Sergi/Desktop/BSC/umls_max15pairs_parents.tsv'\n",
    "data1 = pd.read_csv(file_path, sep='\\t')\n",
    "data1 = data1.sample(frac=1, random_state=42)\n",
    "data1 = data1.reset_index(drop=True)\n",
    "data1_head = data1.head(100)\n",
    "\n",
    "file_path = '/mnt/c/Users/Sergi/Desktop/BSC/umls_max15pairs_parents_and_grandparents.tsv'\n",
    "data2 = pd.read_csv(file_path, sep='\\t')\n",
    "data2 = pd.read_csv(file_path, sep='\\t')\n",
    "data2 = data2.sample(frac=1, random_state=42)\n",
    "data2 = data2.reset_index(drop=True)\n",
    "data2_head = data2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deddcaec-3bcd-4f2d-93f5-fea14a36249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_head = data1_head.reset_index(drop=True)\n",
    "data2_head = data2_head.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e5ee35-f41f-4d7b-b26a-066fbd50fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_head[\"source_target\"]=data1_head[\"source\"] + \" </s> \" + data1_head[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c415d680-a83e-4963-bc3f-107c6cb8cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitTrainer, SetFitModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46acfaa-02a0-465f-8ca7-eaa49ddd1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator MultiOutputClassifier from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = '/mnt/c/Users/Sergi/Desktop/BSC/modelos_entrenados/SetFit/noparents_sp'\n",
    "model = SetFitModel.from_pretrained(path)\n",
    "all_labels = ['BROAD','EXACT','NARROW']\n",
    "\n",
    "def compute_predictions(mention, model):\n",
    "    embeddings = model.model_body.encode([mention], normalize_embeddings=model.normalize_embeddings, convert_to_tensor=True)\n",
    "    predicts = model.model_head.predict_proba(embeddings)\n",
    "    predscores = {all_labels[i]: arr[:,1].tolist()[0] for i, arr in enumerate(predicts)}\n",
    "    top_n_labels = sorted(predscores, key=predscores.get, reverse=True)[:1]\n",
    "    filtered_labels = [label for label in top_n_labels if predscores[label] > 0.5]\n",
    "    return filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1da893-9adf-488e-b61e-6bd0ad242592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import evaluate\n",
    "from collections import Counter\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae36ec5c-e277-4575-947d-0cd3b47b313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_test):\n",
    "    multilabel_f1_metric = evaluate.load(\"f1\", \"multilabel\")\n",
    "    multilabel_accuracy_metric = evaluate.load(\"accuracy\", \"multilabel\")\n",
    "    f1 = multilabel_f1_metric.compute(predictions=y_pred, references=y_test, average=\"micro\")[\"f1\"]\n",
    "    accuracy = multilabel_accuracy_metric.compute(predictions=y_pred, references=y_test)[\"accuracy\"]\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    all_labels = ['BROAD','EXACT','NARROW']\n",
    "    no_label_samples = []\n",
    "    for idx, pred in enumerate(y_pred):\n",
    "        if np.all(pred == 0):\n",
    "            true_labels = [all_labels[i] for i, value in enumerate(y_test[idx]) if value == 1]\n",
    "            no_label_samples.extend(true_labels)\n",
    "\n",
    "    label_counts = Counter(no_label_samples)\n",
    "    label_counts_dict = dict(label_counts)\n",
    "    return {\"f1\": f1, \"accuracy\": accuracy, \"Classes with no given label\": label_counts_dict}\n",
    "\n",
    "def train_evaluate(model, trainX, trainY, testX, testY):\n",
    "    train_dataset, test_dataset = prepare_data(trainX, trainY, testX, testY)\n",
    "    trainer = SetFitTrainer(model=model, train_dataset=train_dataset, eval_dataset=test_dataset, metric=compute_metrics, num_iterations=5)\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    model.save_pretrained('./trained_model')\n",
    "    return metrics\n",
    "\n",
    "def prepare_data(trainX, trainY, testX, testY):\n",
    "    trainY = [{i} for i in trainY]\n",
    "    testY = [{i} for i in testY]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit_transform(trainY)\n",
    "    train_dataset = Dataset.from_dict({\"text\": trainX, \"label\": mlb.fit_transform(trainY)})\n",
    "    test_dataset = Dataset.from_dict({\"text\": testX, \"label\": mlb.transform(testY)})\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336f3e92-6961-41b5-9340-55a8bbde1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = data1_head[\"source_target\"].values.tolist()[:75]\n",
    "trainY = data1_head[\"rel_type\"].values.tolist()[:75]\n",
    "testX = data1_head[\"source_target\"].values.tolist()[75:]\n",
    "testY = data1_head[\"rel_type\"].values.tolist()[75:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22262043-b31e-4ef1-8d0e-5b5b0704ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'list'> <class 'list'>\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 75\n",
      "}) Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "path = '/mnt/c/Users/Sergi/Desktop/BSC/modelos_entrenados/SetFit/noparents_sp'\n",
    "model = SetFitModel.from_pretrained(path, multi_target_strategy=\"multi-output\")\n",
    "all_labels = ['BROAD','EXACT','NARROW']\n",
    "trainY = [{i} for i in trainY]\n",
    "testY = [{i} for i in testY]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(trainY)\n",
    "print(type(trainX), type(mlb.fit_transform(trainY).tolist()), type(testX), type(mlb.fit_transform(testY).tolist()))\n",
    "train_dataset = Dataset.from_dict({\"text\": trainX, \"label\": mlb.fit_transform(trainY).tolist()})\n",
    "test_dataset = Dataset.from_dict({\"text\": testX, \"label\": mlb.transform(testY).tolist()})\n",
    "print(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980eb50-4b83-4698-800e-579c44b4980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Training Pairs: 100%|█████████████| 5/5 [00:00<00:00, 196.30it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 750\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 47\n",
      "  Total train batch size = 16\n",
      "Epoch:   0%|                                          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                     | 0/47 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▌                            | 1/47 [00:10<08:13, 10.74s/it]\u001b[A\n",
      "Iteration:   4%|█▏                           | 2/47 [00:20<07:41, 10.25s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 3/47 [00:29<07:14,  9.87s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 4/47 [00:38<06:54,  9.64s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 5/47 [00:43<05:58,  8.54s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 6/47 [00:51<05:45,  8.43s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 7/47 [00:58<05:28,  8.22s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 8/47 [01:06<05:20,  8.22s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 9/47 [01:15<05:13,  8.24s/it]\u001b[A\n",
      "Iteration:  21%|█████▉                      | 10/47 [01:22<04:59,  8.10s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                     | 11/47 [01:30<04:54,  8.18s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "trainer = SetFitTrainer(model=model, train_dataset=train_dataset, eval_dataset=test_dataset, metric=compute_metrics, num_iterations=5)\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "model.save_pretrained('./trained_model')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cda74e8-4018-436b-9929-c86b63ec90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/mnt/c/Users/Sergi/Desktop/BSC/transformers_rel_mejor', num_labels=4, problem_type=\"multi_label_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2ba1bf-bab4-45aa-8301-75ce2eb223d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_parents_1epoch'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f1668-aab4-4ca3-8623-264e0d18b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Varias seeds con Diversity 0.1: \n",
    " [('t3bn0m0', 0.5104), ('trombo tumoral', 0.4601), ('adenocarcinoma renal', 0.4577), ('oncología médica', 0.4392), ('vena renal trombosada', 0.4362)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea0aadd8-0288-4fcf-9da5-3c4331922730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BROAD': 4.007063865661621, 'EXACT': -4.607285976409912, 'NARROW': -5.324044227600098, 'NO_RELATION': -5.952339172363281}\n",
      "['BROAD']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "term1 = [\"melanoma\"]\n",
    "term2 = [\"linfoma no-hodking estadio IV\"]\n",
    "all_labels = ['BROAD','EXACT','NARROW','NO_RELATION']\n",
    "\n",
    "tokenized_mention = tokenizer(term1, term2, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenized_mention)\n",
    "logits = output.logits\n",
    "predscores = {label: score for label, score in zip(all_labels, logits.tolist()[0])}\n",
    "top_n_labels = sorted(predscores, key=predscores.get, reverse=True)[:4]\n",
    "filtered_labels = [label for label in top_n_labels if predscores[label] > 0]\n",
    "print(predscores)\n",
    "print(filtered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fb961-3283-4050-b5b0-aa7502f39a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf341d07-5677-4b99-a313-3eab96d42ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5e59a-f852-4b61-b232-5b76921805a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f19ff-8374-4b07-b7a0-75bf18e9ac95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93ca78b-5899-4f02-bc3a-d192e43a8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX1 = data1_head[\"source\"].values.tolist()[:750]\n",
    "trainX2 = data1_head[\"target\"].values.tolist()[:750]\n",
    "trainY = data1_head[\"rel_type\"].values.tolist()[:750]\n",
    "testX1 = data1_head[\"source\"].values.tolist()[750:]\n",
    "testX2 = data1_head[\"target\"].values.tolist()[750:]\n",
    "testY = data1_head[\"rel_type\"].values.tolist()[750:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62679d4f-5bf3-4234-80ed-6ce074325d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /mnt/c/Users/Sergi/Desktop/BSC/modelos_entrenados/SetFit/noparents_sp and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "path = '/mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_parents_1epoch'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "batch = tokenizer(\"cáncer de mama\",\"neoplasia maligna de mama\", padding=True, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415313bd-754e-4e84-a30d-e56a61365f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2083, -0.2982]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2c226-6ecb-482a-9b2a-751a7fe82968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7847bb2-c382-4319-b95e-11fa53f70a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergi/Documents/BioTermCategorizer/.env_biotermcategorizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_parents_1epoch and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model_path = '/mnt/c/Users/Sergi/Desktop/BSC/spanish_sapbert_models/sapbert_15_parents_1epoch'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, problem_type=\"multi_label_classification\")\n",
    "\n",
    "# Tokenize text data\n",
    "tokenized_data = tokenizer(trainX1, trainX2, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Convert labels to tensors\n",
    "label_strings = [[i] for i in trainY]\n",
    "# Split the labels into a list\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(label_strings)\n",
    "# Convert labels to tensors\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(tokenized_data.input_ids, tokenized_data.attention_mask, labels)\n",
    "#train_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",  # Output directory\n",
    "    num_train_epochs=3,     # Number of training epochs\n",
    "    per_device_train_batch_size=32,  # Batch size per device\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every steps\n",
    "    save_steps=500,  # Save checkpoint every 500 steps\n",
    "    save_total_limit=2,  # Only keep the last 2 checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item[0] for item in batch]),\n",
    "        'attention_mask': torch.stack([item[1] for item in batch]),\n",
    "        'labels': torch.stack([item[2] for item in batch])\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,  # You can customize data collation if needed\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00592b86-0e2d-4fae-b5a3-550446aa0ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/72 : < :, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170214f-5876-4189-9232-e7184b8fdfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer(trainX1, trainX2, truncation=True, padding=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Convert labels to tensors\n",
    "label_strings = [[i] for i in trainY]\n",
    "# Split the labels into a list\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(label_strings)\n",
    "# Convert labels to tensors\n",
    "labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dataset = TensorDataset(tokenized_data.input_ids, tokenized_data.attention_mask, labels)\n",
    "trainer.predict(testX1,testX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3393e-7a79-4166-8ea0-1fe5e4f9da71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcdd09-5be8-4159-90ba-9893127f2fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f66b9-20cc-4e3c-ad59-e48190fb3ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7deb004-260e-4290-ac46-cf6be6460e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d1bfcd-b160-4d98-a39c-2b037cf73915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement xang_pytextrank (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for xang_pytextrank\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xang_pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2138a8-962f-4335-9d0a-e1fbc0d37388",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xang_pytextrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxang_pytextrank\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpyt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompatibility of systems of linear constraints \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mover the set of natural numbers.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mCriteria of compatibility of a system of linear\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124msupporting set of solutions can be used in solving all \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mthe considered types systems and systems of mixed types.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m phrase,word\u001b[38;5;241m=\u001b[39mpyt\u001b[38;5;241m.\u001b[39mtop_keywords_sentences(text,phrase_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xang_pytextrank'"
     ]
    }
   ],
   "source": [
    "import xang_pytextrank as pyt\n",
    "\n",
    "text=\"Compatibility of systems of linear constraints \\\n",
    "over the set of natural numbers.\\\n",
    "Criteria of compatibility of a system of linear\\\n",
    "Diophantine equations,\\\n",
    "strict inequations, and nonstrict inequations are considered.\\\n",
    "Upper bounds for components of a minimal set of solutions and \\\n",
    "algorithms of construction of minimal generating sets of solutions\\\n",
    "for all types of systems are given. These criteria and the \\\n",
    "corresponding algorithms for constructing a minimal\\\n",
    "supporting set of solutions can be used in solving all \\\n",
    "the considered types systems and systems of mixed types.\"\n",
    "\n",
    "phrase,word=pyt.top_keywords_sentences(text,phrase_limit=15)\n",
    "print('Keywords:',word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9477f-2cfc-469c-baee-2829fc6e3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "terms = []\n",
    "for phrase in doc._.phrases:\n",
    "    terms.append((phrase.text, phrase.rank))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
